{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03b6c56",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install tabulate\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09609",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8d21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running test for parameter set: which_neural_net-5-alpha-0.8_gamma-0.9_epsilon-0.9_epsilon_min-0.01_epsilon_decay-0.995_reward_win-1_punish_lose--1_reward_hit-1_grid_size-20-paddle_step_size-15-backpropagation_state_buffer_size-1\n",
      "INFO:root:Progress: 100.00% Time taken: 0:01:39.843479 for which_neural_net-5-a\n",
      "INFO:root:All parameter tuning runs COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'which_neural_net': 5, 'alpha': 0.8, 'gamma': 0.9, 'epsilon': 0.6179787838684802, 'epsilon_min': 0.01, 'epsilon_decay': 0.995, 'game_board_grid_size': 20, 'paddle_step_size': 15, 'reward_for_winning_episode': 1, 'punishment_for_losing_episode': -1, 'reward_for_hitting_ball': 1, 'backpropagation_state_buffer_size': 1}, 'metrics': {'KR_reward_events_left': 37, 'KR_ball_hits_left': 37, 'KR_avg_episode_length_every_100_episodes': {0: 265.0}, 'KR_avg_epsilon_every_100_episodes': {0: 0.9}, 'KR_max_episode_length': 1259}}\n",
      "{'which_neural_net-5-alpha-0.8_gamma-0.9_epsilon-0.9_epsilon_min-0.01_epsilon_decay-0.995_reward_win-1_punish_lose--1_reward_hit-1_grid_size-20-paddle_step_size-15-backpropagation_state_buffer_size-1': {'config': {'which_neural_net': 5, 'alpha': 0.8, 'gamma': 0.9, 'epsilon': 0.6179787838684802, 'epsilon_min': 0.01, 'epsilon_decay': 0.995, 'game_board_grid_size': 20, 'paddle_step_size': 15, 'reward_for_winning_episode': 1, 'punishment_for_losing_episode': -1, 'reward_for_hitting_ball': 1, 'backpropagation_state_buffer_size': 1}, 'metrics': {'KR_reward_events_left': 37, 'KR_ball_hits_left': 37, 'KR_avg_episode_length_every_100_episodes': {0: 265.0}, 'KR_avg_epsilon_every_100_episodes': {0: 0.9}, 'KR_max_episode_length': 1259}}}\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork_00(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_00, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    \n",
    "class QNetwork_01(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_01, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class QNetwork_02(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_02, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "class QNetwork_03(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_03, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "class QNetwork_04(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_04, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "class QNetwork_05(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_05, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class QNetwork_06(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork_06, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "#Convert input coordinate to discrete grid space.  this smaller grid space should make learning easier.\n",
    "def discretize_grid(coordinate, game_board_grid_size): \n",
    "    return abs(coordinate // game_board_grid_size)\n",
    "\n",
    "def discretize_paddle_step(coordinate, paddle_step_size): \n",
    "    return abs(coordinate // paddle_step_size)\n",
    "\n",
    "#Convert velocity into discretized space (of only 4 options!)\n",
    "def discretize_velocity(velocity_x, velocity_y):\n",
    "    if velocity_x > 0 and velocity_y > 0:\n",
    "        return 0  # Up-Right\n",
    "    elif velocity_x > 0 and velocity_y < 0:\n",
    "        return 1  # Down-Right\n",
    "    elif velocity_x < 0 and velocity_y > 0:\n",
    "        return 2  # Up-Left\n",
    "    elif velocity_x < 0 and velocity_y < 0:\n",
    "        return 3  # Down-Left\n",
    "    \n",
    "def calculate_reward(y_position, reward_for_hitting_ball, height):\n",
    "    min_reward = reward_for_hitting_ball   # Minimum reward at the edges\n",
    "    max_reward = 50 * reward_for_hitting_ball  # Maximum reward at the center\n",
    "    mean = height / 2  # Center of the screen\n",
    "    std_dev = height / 4  # Standard deviation, configurable\n",
    "    gaussian_value = np.exp(-((y_position - mean) ** 2) / (2 * std_dev ** 2))\n",
    "    reward = min_reward + (max_reward - min_reward) * gaussian_value\n",
    "    return reward\n",
    "    \n",
    "# Visualization of the neural network's current recommendation for various paddle positions\n",
    "def draw_q_values_bar(window, model, game_board_grid_size, ball_pos, ball_velocity):\n",
    "    bar_x = 0  # Starting x-coordinate of the bar\n",
    "    bar_width = 50  # Width of the bar\n",
    "    \n",
    "    discretized_ball_x = discretize_grid(ball_pos[0], game_board_grid_size)\n",
    "    discretized_ball_y = discretize_grid(ball_pos[1], game_board_grid_size)\n",
    "    discretized_velocity = discretize_velocity(ball_velocity[0], ball_velocity[1])\n",
    "\n",
    "    for y in range(0, window.get_height(), game_board_grid_size):\n",
    "        discretized_fake_paddle_y = discretize_grid(y, game_board_grid_size)\n",
    "        \n",
    "        # Will have to update this any time I change the definition of state\n",
    "        state = (discretized_fake_paddle_y, discretized_ball_x, discretized_ball_y, discretized_velocity)\n",
    "        state_tensor = torch.FloatTensor([state])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values_output = model(state_tensor)\n",
    "            q_values = q_values_output[0] if len(q_values_output.shape) > 1 else q_values_output\n",
    "\n",
    "        # Find the action with the highest Q-value\n",
    "        max_action = torch.argmax(q_values).item()\n",
    "        \n",
    "        # Set color based on the action\n",
    "        display_action = '-'\n",
    "        if max_action == 0:\n",
    "            color = [255, 0, 0]  # Red for Move Up\n",
    "            display_action = 'U'\n",
    "        elif max_action == 1:\n",
    "            color = [0, 0, 255]  # Blue for Move Down\n",
    "            display_action = 'D'\n",
    "        else:\n",
    "            color = [0, 0, 0]  # Black for Stay Still\n",
    "            display_action = 'S'\n",
    "            \n",
    "        #Pull out the max value\n",
    "        max_q_value = q_values[max_action].item()\n",
    "        \n",
    "        #Pick the color\n",
    "        color = [int(c) for c in color]\n",
    "\n",
    "        # Calculate opacity based on an absolute range\n",
    "        #max_q_value_range_expected = 2.5\n",
    "        #min_q_value_range_expected = -2.5\n",
    "        #min_opacity = 20  # Configurable minimum opacity\n",
    "        #q_value_range = max_q_value_range_expected - min_q_value_range_expected\n",
    "        #normalized_max_q_value = (max_q_value - min_q_value_range_expected) / q_value_range\n",
    "        #opacity = int(normalized_max_q_value * 255)\n",
    "        #opacity = max(min_opacity, min(255, int(opacity)))\n",
    "        \n",
    "        #Calculate opacity based on relative strength of recommended action vs the other actions\n",
    "        #max_q_value_range_expected = 2.5\n",
    "        #min_q_value_range_expected = -2.5\n",
    "        #average_q_value = torch.mean(q_values).item()\n",
    "        #q_value_difference = max_q_value - average_q_value\n",
    "        #max_possible_difference = max_q_value_range_expected - average_q_value\n",
    "        #normalized_difference = q_value_difference / max_possible_difference\n",
    "        #opacity = int(normalized_difference * 255)\n",
    "        #min_opacity = 20  # Configurable minimum opacity\n",
    "        #opacity = max(min_opacity, min(255, int(opacity)))\n",
    "        opacity = 255\n",
    "  \n",
    "        # Create a new surface for the rectangle\n",
    "        rect_surface = pygame.Surface((bar_width, game_board_grid_size), pygame.SRCALPHA)\n",
    "    \n",
    "        try:\n",
    "            # Fill the new surface\n",
    "            rect_surface.fill(tuple(color + [opacity]))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Debug Info - Color: {color}, Opacity: {opacity}\")\n",
    "        \n",
    "        # Fill the new surface\n",
    "        rect_surface.fill(tuple(color + [opacity]))\n",
    "\n",
    "        # Blit the new surface onto the window\n",
    "        window.blit(rect_surface, (bar_x, y))\n",
    "        \n",
    "        # Display the strength of the q-value\n",
    "        if game_board_grid_size >= 20:\n",
    "            font = pygame.font.SysFont(None, 16)\n",
    "            q_value_display = font.render(f\"{display_action}:{max_q_value:.9g}\", True, (255, 255, 255))\n",
    "            window.blit(q_value_display, (5, y + game_board_grid_size // 2))\n",
    "\n",
    "        # Print the Q-values, chosen color, and chosen opacity\n",
    "        #print(f\"Q-values: {q_values.tolist()}, color:{color} opacity:{opacity} max_q_value:{max_q_value}\")\n",
    "\n",
    "#Main Pong game function, accepts key parameters as inputs now\n",
    "def play_de_game(which_neural_net, episodes_to_run, update_screen, watch_every_x_episodes, alpha, gamma, epsilon, epsilon_min, epsilon_decay, game_board_grid_size, paddle_step_size, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball, width, height, backpropagation_state_buffer_size):\n",
    "    \n",
    "    #Instanciate neural net\n",
    "    left_agent_model = {}\n",
    "    input_dim = 4\n",
    "    output_dim = 3\n",
    "    if which_neural_net == 1:\n",
    "        left_agent_model = QNetwork_01(input_dim, output_dim)\n",
    "    elif which_neural_net == 2:\n",
    "        left_agent_model = QNetwork_02(input_dim, output_dim)\n",
    "    elif which_neural_net == 3:\n",
    "        left_agent_model = QNetwork_03(input_dim, output_dim)\n",
    "    elif which_neural_net == 4:\n",
    "        left_agent_model = QNetwork_04(input_dim, output_dim)\n",
    "    elif which_neural_net == 5:\n",
    "        left_agent_model = QNetwork_05(input_dim, output_dim)\n",
    "    else:\n",
    "        left_agent_model = QNetwork_00(input_dim, output_dim)\n",
    "    \n",
    "    #Instanciate optimizer\n",
    "    left_optimizer = optim.Adam(left_agent_model.parameters(), lr=0.1) #default lr=0.001\n",
    "    \n",
    "    # Calculate loss and update neural networks\n",
    "    #loss_fn = nn.MSELoss()\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    \n",
    "    # Initialize buffers for left and right agents\n",
    "    backpropagation_state_buffer_left = []\n",
    "    backpropagation_new_state_buffer_left = []\n",
    "    \n",
    "    #Key Results\n",
    "    KR_max_episode_length = 0\n",
    "    KR_avg_episode_length_every_100_episodes = {}\n",
    "    KR_avg_epsilon_every_100_episodes = {}\n",
    "\n",
    "    # Initialize scores\n",
    "    left_score = 0\n",
    "    right_score = 0\n",
    "    \n",
    "    # Initial paddle positions\n",
    "    #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "    #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "    # Paddle positions to a random spot\n",
    "    left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "    right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "    \n",
    "    # Initial Ball position and velocity\n",
    "    ball_pos = [width // 2, height // 2]\n",
    "    ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "    \n",
    "    # Initialize episode metrics\n",
    "    episode_count = 0\n",
    "    this_episode_length = 0\n",
    "    last_100_episode_lengths = []\n",
    "    last_100_epsilons = []\n",
    "    \n",
    "    # Init whether each AI agent has hit the ball in this episode yet\n",
    "    contact_with_ball_made_this_loop = False\n",
    "    left_player_won = False\n",
    "    right_player_won = False\n",
    "    \n",
    "    # Create the state representation for both agents, this captures the state at the end of the previous loop\n",
    "    state_left = (0, 0, 0, 0)\n",
    "    action_left = 0\n",
    "    action_right = 0\n",
    "    \n",
    "    # Init exploration vars\n",
    "    exploration_direction = None\n",
    "    exploration_length = 0\n",
    "    \n",
    "    # Init results\n",
    "    results = {}\n",
    "    \n",
    "    run = True\n",
    "    user_quit = False\n",
    "    user_paused = False\n",
    "    while run:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "                user_quit = True\n",
    "                \n",
    "        #pygame.time.delay(30)\n",
    "        \n",
    "        #Toggle update_screen\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_v]:\n",
    "            if update_screen:\n",
    "                update_screen = False\n",
    "                pygame.time.delay(100)\n",
    "            else:\n",
    "                update_screen = True\n",
    "                pygame.time.delay(100)\n",
    "        \n",
    "        #Toggle pause status\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_p]:\n",
    "            if user_paused:\n",
    "                user_paused = False\n",
    "                pygame.time.delay(100)\n",
    "            else:\n",
    "                user_paused = True\n",
    "                pygame.time.delay(100)\n",
    "        \n",
    "        #If the game is paused\n",
    "        if user_paused:\n",
    "            continue\n",
    "        \n",
    "        #Track game loops in this episode/game and report to screen to get a sense of how many loops a game lasts\n",
    "        this_episode_length += 1\n",
    "\n",
    "        #Track whether we have a rewarded event in this loop\n",
    "        contact_with_ball_made_this_loop = False\n",
    "        left_player_won = False\n",
    "        right_player_won = False\n",
    "\n",
    "        # Reset rewards to 0 at the beginning of each pass through the game loop\n",
    "        reward_left = 0\n",
    "\n",
    "        # Update the backpropagation buffers (note there's also one for new state)\n",
    "        if len(backpropagation_state_buffer_left) >= backpropagation_state_buffer_size: # If we're at the size limit\n",
    "           backpropagation_state_buffer_left.pop(0)  # Remove the oldest left state\n",
    "        backpropagation_state_buffer_left.append(state_left)  # Add the current left state\n",
    "        \n",
    "        # Convert states to tensors\n",
    "        state_left_tensor = torch.FloatTensor([state_left])\n",
    "    \n",
    "        # Get Q-values from neural network\n",
    "        with torch.no_grad():\n",
    "            q_values_left = left_agent_model(state_left_tensor)\n",
    "            \n",
    "        # Basic mode - Choose actions using epsilon-greedy policy\n",
    "        #action_left = torch.argmax(q_values_left).item() if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "        #if np.random.rand() >= epsilon:\n",
    "        #    action_left = torch.argmax(q_values_left).item()\n",
    "        #else:\n",
    "        #    action_left = np.random.choice(action_space)\n",
    "        \n",
    "        # More advanced exploration algo\n",
    "        if np.random.rand() >= epsilon:\n",
    "            action_left = torch.argmax(q_values_left).item()\n",
    "            exploration_direction = None  # Reset exploration\n",
    "        else:\n",
    "            if exploration_direction is None or exploration_length == 0:\n",
    "                # Start a new exploration\n",
    "                exploration_direction = np.random.choice(action_space)\n",
    "                exploration_length = np.random.randint(5, height // game_board_grid_size)  # Random length between 5 and a num that will let paddle traverse entire grid\n",
    "            else:\n",
    "                # Continue existing exploration\n",
    "                exploration_length -= 1\n",
    "            action_left = exploration_direction\n",
    "        \n",
    "        # Manual human paddle movement with boundary checks\n",
    "        keys = pygame.key.get_pressed()\n",
    "        #if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "        #   left_paddle_pos[1] -= paddle_step_size\n",
    "        #if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "        #   left_paddle_pos[1] += paddle_step_size\n",
    "        #if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "        #   right_paddle_pos[1] -= paddle_step_size\n",
    "        #if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "        #   right_paddle_pos[1] += paddle_step_size\n",
    "        \n",
    "        # Ideal Paddle Control of Right paddle\n",
    "        paddle_center_offset = paddle_height / 2\n",
    "        ideal_paddle_y = ball_pos[1] - paddle_center_offset\n",
    "        right_paddle_center = right_paddle_pos[1] + paddle_center_offset\n",
    "        if ideal_paddle_y > right_paddle_center and right_paddle_pos[1] < height - paddle_height:\n",
    "            right_paddle_pos[1] += paddle_step_size # Move Down\n",
    "        elif ideal_paddle_y < right_paddle_center and right_paddle_pos[1] > 0:\n",
    "            right_paddle_pos[1] -= paddle_step_size # Move Up\n",
    "        # else: Stay put (no code needed for this)\n",
    "        \n",
    "        #Left AI agent moves the paddle!! \n",
    "        if action_left == 0 and left_paddle_pos[1] > 0:  # Move Up\n",
    "            left_paddle_pos[1] -= paddle_step_size \n",
    "        elif action_left == 1 and left_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            left_paddle_pos[1] += paddle_step_size  \n",
    "        #elif action_left == 2: \n",
    "            # Stay Still, so no movement\n",
    "            \n",
    "        # Debugging code to print current state and action for both agents\n",
    "        #print(f\"Current State Left: {state_left}, Action Taken Left: {action_left}\")\n",
    "        #print(f\"Current State Right: {state_right}, Action Taken Right: {action_right}\")\n",
    "\n",
    "        # Update ball position\n",
    "        ball_pos[0] += ball_velocity[0]\n",
    "        ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "        # Collision detection with walls\n",
    "        if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "            ball_velocity[1] = -ball_velocity[1]\n",
    "              \n",
    "        # Collision detection with paddles\n",
    "        collision_offset = 5  # Define an offset to push the ball away from the paddle\n",
    "        if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "            left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] += collision_offset  # Push the ball away from the paddle\n",
    "            reward_left += calculate_reward(discretize_paddle_step(left_paddle_pos[1], paddle_step_size), reward_for_hitting_ball, height)  # Add reward for left agent\n",
    "            contact_with_ball_made_this_loop = True\n",
    "        elif (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "              right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] -= collision_offset  # Push the ball away from the paddle     \n",
    "\n",
    "        # Ball reset, scoring, and immediate feedback game-over condition\n",
    "        episode_just_ended = False\n",
    "        if ball_pos[0] < 0:\n",
    "            right_player_won = True\n",
    "            episode_just_ended = True\n",
    "            right_score += 1  # Right player scores\n",
    "            reward_left += punishment_for_losing_episode  # Punishment for the left agent\n",
    "        elif ball_pos[0] > width:\n",
    "            left_player_won = True\n",
    "            episode_just_ended = True\n",
    "            left_score += 1  # Left player scores\n",
    "            reward_left += reward_for_winning_episode\n",
    "                \n",
    "        #All the end-of-episode stuff\n",
    "        if episode_just_ended:\n",
    "            #Record some key metrics to help optimize later\n",
    "            if this_episode_length > KR_max_episode_length: \n",
    "                KR_max_episode_length = this_episode_length\n",
    "            last_100_episode_lengths.append(this_episode_length)\n",
    "            if episode_count % 100 == 0: # Only record avg episode length every 100 eps\n",
    "                last_100_episode_avg = sum(last_100_episode_lengths) / len(last_100_episode_lengths)\n",
    "                KR_avg_episode_length_every_100_episodes[episode_count] = last_100_episode_avg\n",
    "                last_100_episode_lengths = [] # and reset the rolling history\n",
    "            last_100_epsilons.append(epsilon)\n",
    "            if episode_count % 100 == 0: # Only record avg epsilons every 100 eps\n",
    "                last_100_epsilon_avg = sum(last_100_epsilons) / len(last_100_epsilons)\n",
    "                KR_avg_epsilon_every_100_episodes[episode_count] = last_100_epsilon_avg\n",
    "                last_100_epsilons = [] # and reset the rolling history   \n",
    "            # Reset paddle positions to the middle\n",
    "            left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            #left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #ball_velocity = [-4, 4] # For testing, always left/down   \n",
    "            if epsilon > epsilon_min: # Decay epsilon at the end of a game/episode\n",
    "                epsilon *= epsilon_decay    \n",
    "            episode_count += 1  # Increment episode count\n",
    "            this_episode_length = 0 # Reset length of episode\n",
    "            \n",
    "         # After taking an action, observe new state and reward\n",
    "        #print('NEW STATE ', 'pad_y/disc:', left_paddle_pos[1], ':', discretize_paddle_step(left_paddle_pos[1], paddle_step_size), ' ball_x/disc:', ball_pos[0], ':', discretize_grid(ball_pos[0], game_board_grid_size), ' ball_y/disc:', ball_pos[1], ':', discretize_grid(ball_pos[1], game_board_grid_size), ' vel/disc:', ball_velocity[0], ':', ball_velocity[1], '->', discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        new_state_left = (discretize_paddle_step(left_paddle_pos[1], paddle_step_size), discretize_grid(ball_pos[0], game_board_grid_size), discretize_grid(ball_pos[1], game_board_grid_size), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        #new_state_left = (discretize_paddle_step(left_paddle_pos[1], paddle_step_size), 0, discretize_grid(ball_pos[1], game_board_grid_size), 0)\n",
    "        \n",
    "        \n",
    "        # Update the backpropagation buffers for new_state\n",
    "        if len(backpropagation_new_state_buffer_left) >= backpropagation_state_buffer_size: # If we're at the size limit\n",
    "           backpropagation_new_state_buffer_left.pop(0)  # Remove the oldest left state\n",
    "        backpropagation_new_state_buffer_left.append(new_state_left)  # Add the current left state\n",
    "        \n",
    "        # Simple Backpropagation to NN\n",
    "        if new_state_left != state_left:\n",
    "            #print(' episode: ', episode_count, 'length: ', this_episode_length, ' reward_left: ', reward_left, ' state: ', state_left, ' new_state:', new_state_left)\n",
    "            target_left = reward_left + gamma * torch.max(left_agent_model(torch.FloatTensor([new_state_left])))\n",
    "            predicted_left = left_agent_model(state_left_tensor)[0][action_left]\n",
    "            loss_left = loss_fn(predicted_left, target_left)\n",
    "            left_optimizer.zero_grad()\n",
    "            loss_left.backward()\n",
    "            left_optimizer.step()\n",
    "        #else:\n",
    "            #print(' episode: ', episode_count, 'length: ', this_episode_length, ' reward_left: ', reward_left, ' state: ', state_left, ' new_state:', new_state_left, ' <- state unchanged');\n",
    "        \n",
    "        # Proximity reward: check if left state has changed or reward was applied\n",
    "        proximity_min_reward = 0.0001\n",
    "        proximity_max_reward = 1.0\n",
    "        if new_state_left != state_left or reward_left != 0:\n",
    "            # Proximity rewards, for getting the paddle close to \n",
    "            paddle_center_y_left = discretize_paddle_step(left_paddle_pos[1], paddle_step_size) + paddle_height / 2  \n",
    "            abs_distance_left = abs(discretize_grid(ball_pos[0], game_board_grid_size) - discretize_paddle_step(paddle_center_y_left, paddle_step_size))\n",
    "            scaled_reward_left = proximity_min_reward + (proximity_max_reward - proximity_min_reward) * (abs_distance_left / height) # Scale the reward linearly with the distance\n",
    "            scaled_reward_left = min(max(scaled_reward_left, proximity_min_reward), proximity_max_reward)\n",
    "            reward_left += scaled_reward_left\n",
    "    \n",
    "        # Backpropagation to NN using buffer, only rewarding for the reward event\n",
    "        #if reward_left != 0:\n",
    "        #    # Replay state buffer\n",
    "        #    for idx, state_bp_tmp_left in enumerate(backpropagation_state_buffer_left):\n",
    "        #        state_bp_tmp_left_tensor = torch.FloatTensor([state_bp_tmp_left])\n",
    "        #        # Apply reward only to the most recent frame\n",
    "        #        target_left = (reward_left if idx == len(backpropagation_state_buffer_left) - 1 else 0) + gamma * torch.max(left_agent_model(torch.FloatTensor([new_state_left])))\n",
    "        #        predicted_left = left_agent_model(state_bp_tmp_left_tensor)[0][action_left]\n",
    "        #       loss_left = loss_fn(predicted_left, target_left)\n",
    "        #        left_optimizer.zero_grad()\n",
    "        #        loss_left.backward()\n",
    "        #        left_optimizer.step()\n",
    "            \n",
    "        #if contact_with_ball_made_this_loop:\n",
    "        #    #print('START REPLAYING BUFFER DUE TO CONTACT WITH PADDLE')\n",
    "        #    def polynomial_decay(idx, max_idx, exponent=2): \n",
    "        #        return ((max_idx - idx) / max_idx) ** exponent\n",
    "        #    buffer_length = len(backpropagation_state_buffer_left)\n",
    "        #    for idx, (state_bp_tmp_left, next_state_tmp_left) in enumerate(zip(backpropagation_state_buffer_left, backpropagation_new_state_buffer_left)):\n",
    "        #        state_bp_tmp_left_tensor = torch.FloatTensor([state_bp_tmp_left])\n",
    "        #        weighted_reward_left = reward_left * polynomial_decay(buffer_length - 1 - idx, buffer_length - 1) # Polynomial decay\n",
    "        #        #weighted_reward_left = reward_left\n",
    "        #        #print(' ep: ', episode_count, 'loop: ', this_episode_length, ' weighted_reward: ', weighted_reward_left, ' state:', state_bp_tmp_left, ' next_state: ', next_state_tmp_left)\n",
    "        #        target_left = weighted_reward_left + gamma * torch.max(left_agent_model(torch.FloatTensor([next_state_tmp_left]))) \n",
    "        #        predicted_left = left_agent_model(state_bp_tmp_left_tensor)[0][action_left]\n",
    "        #        loss_left = loss_fn(predicted_left, target_left)\n",
    "        #        left_optimizer.zero_grad()\n",
    "        #        loss_left.backward()\n",
    "        #        left_optimizer.step()\n",
    "          \n",
    "        #if right_player_won:\n",
    "        #    #print('START REPLAYING BUFFER DUE TO LEFT PLAYER LOSS')\n",
    "        #    def polynomial_decay(idx, max_idx, exponent=2): \n",
    "        #        return ((max_idx - idx) / max_idx) ** exponent\n",
    "        #    buffer_length = len(backpropagation_state_buffer_left)\n",
    "        #    for idx, (state_bp_tmp_left, next_state_tmp_left) in enumerate(zip(backpropagation_state_buffer_left, backpropagation_new_state_buffer_left)):\n",
    "        #        state_bp_tmp_left_tensor = torch.FloatTensor([state_bp_tmp_left])\n",
    "        #        weighted_reward_left = reward_left * polynomial_decay(buffer_length - 1 - idx, buffer_length - 1)\n",
    "        #        #print(' ep: ', episode_count, 'loop: ', this_episode_length, ' weighted_reward: ', weighted_reward_left, ' state:', state_bp_tmp_left, ' next_state: ', next_state_tmp_left)\n",
    "        #        target_left = weighted_reward_left + gamma * torch.max(left_agent_model(torch.FloatTensor([next_state_tmp_left])))\n",
    "        #        predicted_left = left_agent_model(state_bp_tmp_left_tensor)[0][action_left]\n",
    "        #        loss_left = loss_fn(predicted_left, target_left)\n",
    "        #        left_optimizer.zero_grad()\n",
    "        #        loss_left.backward()\n",
    "        #        left_optimizer.step()\n",
    "\n",
    "        # Update current state for next iteration\n",
    "        state_left = new_state_left\n",
    "        \n",
    "        #Intermittent visualization so that i can build intuition but run fast episodes between\n",
    "        #if episode_count % watch_every_x_episodes == 0:\n",
    "        #    intermittent_visualization_is_on = True\n",
    "        #else:\n",
    "        #    intermittent_visualization_is_on = False\n",
    "\n",
    "        if update_screen and episode_count % watch_every_x_episodes == 0:\n",
    "            # Draw paddles, ball, and scores\n",
    "            window.fill((0, 0, 0))  # Clear screen\n",
    "            pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "            pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "            pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "            # Display scores\n",
    "            font = pygame.font.SysFont(None, 20)\n",
    "            score_display = font.render(f\"score: {left_score} - {right_score}\", True, (255, 255, 255))\n",
    "            window.blit(score_display, (width // 2 - 40, 10))\n",
    "\n",
    "            # Display episode count\n",
    "            font = pygame.font.SysFont(None, 20)\n",
    "            episode_display = font.render(f\"episodes played: {episode_count}\", True, (255, 255, 255))\n",
    "            window.blit(episode_display, (width // 2 - 50, 30))\n",
    "            \n",
    "            # Display epsilon\n",
    "            font = pygame.font.SysFont(None, 20)\n",
    "            epsilon_display = font.render(f\"epsilon:{epsilon:.2g}\", True, (255, 255, 255))\n",
    "            window.blit(epsilon_display, (width // 2 - 50, 50))\n",
    "            \n",
    "            #Visualize neural net recommendations for various paddle locations\n",
    "            draw_q_values_bar(window, left_agent_model, game_board_grid_size, ball_pos, ball_velocity)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "        if episode_count > episodes_to_run and episodes_to_run > 0:\n",
    "            run = False\n",
    "    \n",
    "    results = {\n",
    "        'config': {\n",
    "            'which_neural_net': which_neural_net,\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'epsilon': epsilon,\n",
    "            'epsilon_min': epsilon_min,\n",
    "            'epsilon_decay': epsilon_decay,\n",
    "            'game_board_grid_size': game_board_grid_size,\n",
    "            'paddle_step_size': paddle_step_size,\n",
    "            'reward_for_winning_episode': reward_for_winning_episode,\n",
    "            'punishment_for_losing_episode': punishment_for_losing_episode,\n",
    "            'reward_for_hitting_ball': reward_for_hitting_ball,\n",
    "            'backpropagation_state_buffer_size': backpropagation_state_buffer_size\n",
    "        },\n",
    "        'metrics': {\n",
    "            'KR_avg_episode_length_every_100_episodes': KR_avg_episode_length_every_100_episodes,\n",
    "            'KR_avg_epsilon_every_100_episodes': KR_avg_epsilon_every_100_episodes,\n",
    "            'KR_max_episode_length': KR_max_episode_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    return results, user_quit, left_agent_model\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('AI Learns Pong')\n",
    "\n",
    "# Initialize a dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Flag to check if the user wants to quit\n",
    "user_quit = False\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "# Define ranges and step sizes for key parameters\n",
    "which_neural_net_range = [5] # [0, 1, 2, 3, 4, 5, 6] different size neural nets, hard-coded above\n",
    "alpha_range = [0.8] # [0.1, 0.15, 0.2] If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. \n",
    "gamma_range = [0.9] # [0.9, 0.95, 0.99] If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "epsilon_range = [.9] # [0.8, 0.9, 1.0]\n",
    "epsilon_min_range = [0.01] # [0.05, 0.1, 0.15]\n",
    "epsilon_decay_range = [0.995] # [0.995, 0.999, 0.9999] \n",
    "reward_for_winning_episode_range = [1] # [1, 2]\n",
    "punishment_for_losing_episode_range = [-1] # [-1, -2]\n",
    "reward_for_hitting_ball_range = [1] # [0.5, 1, 1.5]\n",
    "game_board_grid_size_range = [20] # [10, 50, 100]\n",
    "paddle_step_size_range = [15] # [5, 10, 15]\n",
    "backpropagation_state_buffer_size_range = [1] # [5, 10, 15]\n",
    "\n",
    "# For progress bar\n",
    "total_runs = len(which_neural_net_range) * len(alpha_range) * len(gamma_range) * len(epsilon_range) * len(epsilon_min_range) * len(epsilon_decay_range) * len(reward_for_winning_episode_range) * len(punishment_for_losing_episode_range) * len(reward_for_hitting_ball_range) * len(game_board_grid_size_range) * len(paddle_step_size_range) * len(backpropagation_state_buffer_size_range)\n",
    "completed_runs = 0\n",
    "\n",
    "#Constants\n",
    "DATA_FILE_PREFIX = 'v66-PyTorch-'\n",
    "update_screen = True\n",
    "watch_every_x_episodes = 1 # 1 to watch all\n",
    "save_models = True\n",
    "\n",
    "# TODO record equal numbers of rewards (paddle hits)\n",
    "# Nested loops to iterate through each parameter combination\n",
    "for alpha in alpha_range:\n",
    "    for gamma in gamma_range:\n",
    "        for epsilon in epsilon_range:\n",
    "            for epsilon_min in epsilon_min_range:\n",
    "                for epsilon_decay in epsilon_decay_range:\n",
    "                    for reward_for_winning_episode in reward_for_winning_episode_range:\n",
    "                        for punishment_for_losing_episode in punishment_for_losing_episode_range:\n",
    "                            for reward_for_hitting_ball in reward_for_hitting_ball_range:\n",
    "                                for paddle_step_size in paddle_step_size_range:\n",
    "                                    for backpropagation_state_buffer_size in backpropagation_state_buffer_size_range:\n",
    "                                        for game_board_grid_size in game_board_grid_size_range:\n",
    "                                            for which_neural_net in which_neural_net_range:\n",
    "\n",
    "                                                if user_quit:\n",
    "                                                    break\n",
    "\n",
    "                                                # Generate a unique identifier for this parameter combination\n",
    "                                                param_id = f\"which_neural_net-{which_neural_net}-alpha-{alpha}_gamma-{gamma}_epsilon-{epsilon}_epsilon_min-{epsilon_min}_epsilon_decay-{epsilon_decay}_reward_win-{reward_for_winning_episode}_punish_lose-{punishment_for_losing_episode}_reward_hit-{reward_for_hitting_ball}_grid_size-{game_board_grid_size}-paddle_step_size-{paddle_step_size}-backpropagation_state_buffer_size-{backpropagation_state_buffer_size}\"\n",
    "                                                logging.info(f\"Running test for parameter set: {param_id}\")\n",
    "                                                start_time = datetime.datetime.now()\n",
    "\n",
    "                                                # Run the game with the current parameter combination\n",
    "                                                episodes_to_run = 5000\n",
    "                                                results, user_quit, left_agent_model = play_de_game(which_neural_net, episodes_to_run, update_screen, watch_every_x_episodes, alpha, gamma, epsilon, epsilon_min, epsilon_decay, game_board_grid_size, paddle_step_size, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball, width, height, backpropagation_state_buffer_size)\n",
    "\n",
    "                                                # Store the results in the all_results dictionary\n",
    "                                                all_results[param_id] = results\n",
    "                                                #logging.info(f\"Completed test for parameter set: {param_id}\")\n",
    "\n",
    "                                                if save_models:\n",
    "                                                    torch.save(left_agent_model.state_dict(), 'data/'+DATA_FILE_PREFIX+param_id+'-left_agent_model_state_dict.pth')\n",
    "\n",
    "                                                completed_runs += 1\n",
    "                                                percent_complete = (completed_runs / total_runs) * 100\n",
    "                                                end_time = datetime.datetime.now()\n",
    "                                                time_taken = end_time - start_time\n",
    "                                                logging.info(f\"Progress: {percent_complete:.2f}% Time taken: {time_taken} for {param_id[:20]}\")\n",
    "\n",
    "                                            if user_quit:\n",
    "                                                break\n",
    "                                        if user_quit:\n",
    "                                            break\n",
    "                                    if user_quit:\n",
    "                                        break\n",
    "                                if user_quit:\n",
    "                                    break\n",
    "                            if user_quit:\n",
    "                                break\n",
    "                        if user_quit:\n",
    "                            break\n",
    "                    if user_quit:\n",
    "                        break\n",
    "                if user_quit:\n",
    "                    break\n",
    "            if user_quit:\n",
    "                break\n",
    "        if user_quit:\n",
    "            break\n",
    "    if user_quit:\n",
    "        break\n",
    "\n",
    "\n",
    "# Save all_results to a pickle file\n",
    "#if not user_quit:\n",
    "with open(\"data/\"+DATA_FILE_PREFIX+\"ParamTest-All-Results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "    \n",
    "print(all_results)\n",
    "\n",
    "logging.info(\"All parameter tuning runs COMPLETED.\")\n",
    "        \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf606",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4466de3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/v66-PyTorch-ParamTest-All-Results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data from the pickle file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/v66-PyTorch-ParamTest-All-Results.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     all_results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize empty lists to store config and metrics data\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox (Personal)/AI/PongReinforcementLearning/PongReinforcementLearningVENV/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/v66-PyTorch-ParamTest-All-Results.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open(\"data/v66-PyTorch-ParamTest-All-Results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "# Initialize empty lists to store config and metrics data\n",
    "config_data = []\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate through the all_results dictionary to separate config and metrics\n",
    "for param_id, result in all_results.items():\n",
    "    config = result['config']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Add a parameter ID to link config and metrics\n",
    "    config['param_id'] = param_id\n",
    "    metrics['param_id'] = param_id\n",
    "    \n",
    "    config_data.append(config)\n",
    "    metrics_data.append(metrics)\n",
    "\n",
    "# Convert lists of dictionaries to DataFrames\n",
    "config_df = pd.DataFrame(config_data)\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Merge the config and metrics DataFrames on 'param_id'\n",
    "df = pd.merge(config_df, metrics_df, on='param_id')\n",
    "\n",
    "# Identify and sort the top runs by max average episode length over 100 episodes\n",
    "df['max_avg_episode_length'] = df['KR_avg_episode_length_every_100_episodes'].apply(lambda x: max(x.values()))\n",
    "sorted_df = df.sort_values(by='max_avg_episode_length', ascending=False)\n",
    "\n",
    "# Take the top 5 runs\n",
    "top_5_runs = sorted_df.head(5)\n",
    "\n",
    "# Create a new figure for the plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a secondary y-axis for the average epsilon values\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Initialize lists to store config and metrics for text summary\n",
    "config_list = []\n",
    "metrics_list = []\n",
    "\n",
    "# Loop through the top 5 runs to plot and collect summaries\n",
    "for index, row in top_5_runs.iterrows():\n",
    "    # Extract the config and metrics for this run\n",
    "    config = row[['which_neural_net', 'alpha', 'gamma', 'epsilon', 'game_board_grid_size', 'backpropagation_state_buffer_size']]\n",
    "    metrics_length = row['KR_avg_episode_length_every_100_episodes']\n",
    "    metrics_epsilon = row['KR_avg_epsilon_every_100_episodes']\n",
    "    \n",
    "    # Generate a short identifier for the run\n",
    "    short_id = f\"Run_{index+1}\"\n",
    "    \n",
    "    # Plotting episode length on the primary y-axis\n",
    "    ax1.plot(list(metrics_length.keys()), list(metrics_length.values()), label=f\"{short_id} - Episode Length\")\n",
    "    \n",
    "    # Plotting average epsilon on the secondary y-axis\n",
    "    ax2.plot(list(metrics_epsilon.keys()), list(metrics_epsilon.values()), linestyle='--', label=f\"{short_id} - Avg Epsilon\")\n",
    "    \n",
    "    # Collect data for Text Summary\n",
    "    config['Run_ID'] = short_id\n",
    "    config_list.append(config)\n",
    "    metrics_list.append({\"Run_ID\": short_id, \"Last_Avg_Ep_Length\": list(metrics_length.values())[-1]})\n",
    "\n",
    "# Add legend and labels to the primary y-axis\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_xlabel('Episode Count')\n",
    "ax1.set_ylabel('Average Episode Length over 100 Episodes')\n",
    "ax1.set_title(\"Performance of Top 5 Runs\")\n",
    "\n",
    "# Add legend and labels to the secondary y-axis\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Average Epsilon over 100 Episodes')\n",
    "\n",
    "# Set the limits for the secondary y-axis\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print out the text summaries\n",
    "print(\"Top Performing Runs:\")\n",
    "print(tabulate(config_list, headers=\"keys\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76598079",
   "metadata": {},
   "source": [
    "# Parameter Tuning and Analysis for Reinforcement Learning in Pong\n",
    "\n",
    "## Refactoring the Code\n",
    "\n",
    "Initially, the Pong game was implemented in a monolithic script. To make it more modular and facilitate parameter tuning, the following steps were taken:\n",
    "\n",
    "1. **Encapsulation**: The core game logic was encapsulated into a function called `play_de_game()`.\n",
    "2. **Parameterization**: The function was designed to accept various hyperparameters as arguments, allowing for easy tuning. These parameters include:\n",
    "    - `alpha`: The learning rate\n",
    "    - `gamma`: The discount factor\n",
    "    - `epsilon`: The exploration rate\n",
    "    - `epsilon_min`: The minimum exploration rate\n",
    "    - `epsilon_decay`: The decay rate for `epsilon`\n",
    "    - `GAME_BOARD_GRID_SIZE`: The size of the game board grid\n",
    "    - `reward_for_winning_episode`: The reward for winning an episode\n",
    "    - `punishment_for_losing_episode`: The punishment for losing an episode\n",
    "    - `reward_for_hitting_ball`: The reward for hitting the ball\n",
    "\n",
    "## Running the Tests\n",
    "\n",
    "After refactoring, the game was set up to run over a thousand tests, each with 2500 episodes. The tests were designed to explore a wide range of hyperparameters:\n",
    "\n",
    "- **Alpha**: Learning rate, affecting how quickly the agent adapts to new information.\n",
    "- **Gamma**: Discount factor, influencing how much future rewards are considered.\n",
    "- **Epsilon**: Exploration rate, determining the likelihood of taking a random action.\n",
    "- **Epsilon Min**: The minimum value that `epsilon` can decay to.\n",
    "- **Epsilon Decay**: The rate at which `epsilon` decays over time.\n",
    "- **Game Board Grid Size**: Affects the complexity of the state space.\n",
    "- **Reward for Winning Episode**: Encourages the agent to win.\n",
    "- **Punishment for Losing Episode**: Discourages the agent from losing.\n",
    "- **Reward for Hitting Ball**: Encourages the agent to hit the ball.\n",
    "\n",
    "The results of each test run were stored in a Python dictionary and then serialized to a pickle file (`all_results.pkl`) for later analysis.\n",
    "\n",
    "## Data Analysis Plan\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Load Data**: Import the `all_results.pkl` file into a Pandas DataFrame.\n",
    "2. **Data Cleaning**: Remove any missing values and outliers, and convert columns to appropriate data types.\n",
    "3. **Exploratory Data Analysis (EDA)**: Use statistical and visual methods to understand the data's underlying structure.\n",
    "4. **Performance Metrics**: Evaluate the performance of different parameter sets based on metrics like average reward, episodes to convergence, etc.\n",
    "\n",
    "### Expected Insights:\n",
    "\n",
    "- **Optimal Parameters**: Identify the set of parameters that yield the best performance.\n",
    "- **Parameter Sensitivity**: Understand how sensitive the model's performance is to changes in individual parameters.\n",
    "- **Convergence Behavior**: Analyze how quickly the agent learns optimal policies under different parameter settings.\n",
    "- **Reward Dynamics**: Examine how different reward structures affect the agent's learning process.\n",
    "\n",
    "By the end of this analysis, I expect to have a comprehensive understanding of how different hyperparameters affect the learning process and performance of the reinforcement learning agent in the Pong game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2739a9",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462cd2",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0693c8",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as ε-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2002e88",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e387d6",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d646240",
   "metadata": {},
   "source": [
    "## Building intuition around training variables\n",
    "\n",
    "1. **Alpha (α) - Learning Rate**: \n",
    "    - **What it does**: Determines how much of the new Q-value estimate I adopt.\n",
    "    - **Intuition**: Think of it as a \"blending factor.\" If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. A value between 0 and 1 blends the old and new information.\n",
    "    - **Example**: If α is high (closer to 1), I will rapidly adapt to new strategies but may also forget useful past knowledge quickly.\n",
    "\n",
    "2. **Gamma (γ) - Discount Factor**: \n",
    "    - **What it does**: Influences how much future rewards contribute to the Q-value.\n",
    "    - **Intuition**: It's like a \"patience meter.\" A high γ makes me prioritize long-term reward over short-term reward.\n",
    "    - **Example**: If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "\n",
    "3. **Epsilon (ε) - Exploration Rate**: \n",
    "    - **What it does**: Controls the trade-off between exploration (trying new actions) and exploitation (sticking with known actions).\n",
    "    - **Intuition**: It's like the \"curiosity level.\" A high ε encourages me to try new things, while a low ε makes me stick to what I know.\n",
    "    - **Example**: If ε starts high and decays over time (ε-decay), I will initially explore a lot and gradually shift to exploiting my learned knowledge.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
