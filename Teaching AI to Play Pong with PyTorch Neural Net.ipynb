{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03b6c56",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install tabulate\n",
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e54589",
   "metadata": {},
   "source": [
    "# See if I can run an external Pygame window from a Jupyter notebook on macosx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecbdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create external window\n",
    "win = pygame.display.set_mode((500, 500))\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(100)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    # Game logic here (e.g., move a rectangle)\n",
    "    pygame.draw.rect(win, (255, 0, 0), (250, 250, 50, 50))\n",
    "    \n",
    "    pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae897a",
   "metadata": {},
   "source": [
    "**Well, it runs but shutdown isn't graceful.  The window pops up, draws a glorious red square.  But then simple window commands like \"close\" fail.  I had to Force Quit which then also brought the Jupyter notebook kernel to the ground.  This may wind up being a royal PITA but i'll give it a shot for now.  Worst case I'll switch to a simple python script run from the console.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09609",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8d21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running test for parameter set: alpha-0.5_gamma-0.9999_epsilon-1.0_epsilon_min-0.1_epsilon_decay-0.99999_reward_win-1_punish_lose--5_reward_hit-5_grid_size-50\n",
      "INFO:root:Progress: 100.00% Time taken: 0:07:37.464022 for alpha-0.5_gamma-0.99\n",
      "INFO:root:All parameter tuning runs completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'alpha': 0.5, 'gamma': 0.9999, 'epsilon': 0.9858214052138287, 'epsilon_min': 0.1, 'epsilon_decay': 0.99999, 'game_board_grid_size': 50, 'reward_for_winning_episode': 1, 'punishment_for_losing_episode': -5, 'reward_for_hitting_ball': 5}, 'metrics': {'KR_reward_events_left': 889, 'KR_reward_events_right': 839, 'KR_ball_hits_left': 157, 'KR_ball_hits_right': 143, 'KR_max_episode_length': 761}}\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "#Convert input coordinate to discrete grid space.  this smaller grid space should make learning easier.\n",
    "def discretize_grid(coordinate, game_board_grid_size): \n",
    "    return coordinate // game_board_grid_size\n",
    "\n",
    "#Convert velocity into discretized space (of only 4 options!)\n",
    "def discretize_velocity(velocity_x, velocity_y):\n",
    "    if velocity_x > 0 and velocity_y > 0:\n",
    "        return 0  # Up-Right\n",
    "    elif velocity_x > 0 and velocity_y < 0:\n",
    "        return 1  # Down-Right\n",
    "    elif velocity_x < 0 and velocity_y > 0:\n",
    "        return 2  # Up-Left\n",
    "    elif velocity_x < 0 and velocity_y < 0:\n",
    "        return 3  # Down-Left\n",
    "    \n",
    "#Main Pong game function, accepts key parameters as inputs now\n",
    "def play_de_game(episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, game_board_grid_size, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball):\n",
    "    \n",
    "    #Instanciate our Neural Nets + Optimizers\n",
    "    left_agent_model = QNetwork(input_dim=4, output_dim=3)\n",
    "    right_agent_model = QNetwork(input_dim=4, output_dim=3)\n",
    "    left_optimizer = optim.Adam(left_agent_model.parameters(), lr=0.001)\n",
    "    right_optimizer = optim.Adam(right_agent_model.parameters(), lr=0.001)\n",
    "    \n",
    "    #Key Results\n",
    "    KR_reward_events_left = 0\n",
    "    KR_reward_events_right = 0\n",
    "    KR_ball_hits_left = 0\n",
    "    KR_ball_hits_right = 0\n",
    "    KR_max_episode_length = 0\n",
    "\n",
    "    # Initialize scores\n",
    "    left_score = 0\n",
    "    right_score = 0\n",
    "    \n",
    "    # Initial paddle positions\n",
    "    left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "    right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "    # Paddle positions to a random spot\n",
    "    #left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "    #right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "    \n",
    "    # Initial Ball position and velocity\n",
    "    ball_pos = [width // 2, height // 2]\n",
    "    ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "    \n",
    "    # Initialize episode counters\n",
    "    episode_count = 0\n",
    "    this_episode_length = 0\n",
    "    \n",
    "    # Init whether each AI agent has hit the ball in this episode yet\n",
    "    contact_with_ball_made_this_episode_left = False\n",
    "    contact_with_ball_made_this_episode_right = False\n",
    "    \n",
    "    # Init results\n",
    "    results = {}\n",
    "    \n",
    "    run = True\n",
    "    user_quit = False\n",
    "    while run:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "                user_quit = True\n",
    "\n",
    "        #Track game loops in this episode/game and report to screen to get a sense of how many loops a game lasts\n",
    "        this_episode_length += 1\n",
    "\n",
    "        #Debug track whether we have a rewarded event in this loop\n",
    "        reward_applied_this_loop = False\n",
    "\n",
    "        # Reset rewards to 0 at the beginning of each pass through the game loop\n",
    "        reward_left = 0\n",
    "        reward_right = 0\n",
    "\n",
    "        # Create the state representation for both agents\n",
    "        state_left = (left_paddle_pos[1], discretize_grid(ball_pos[0], game_board_grid_size), discretize_grid(ball_pos[1], game_board_grid_size), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        state_right = (right_paddle_pos[1], discretize_grid(ball_pos[0], game_board_grid_size), discretize_grid(ball_pos[1], game_board_grid_size), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Convert states to tensors\n",
    "        state_left_tensor = torch.FloatTensor([state_left])\n",
    "        state_right_tensor = torch.FloatTensor([state_right])\n",
    "\n",
    "        # Get Q-values from neural networks\n",
    "        with torch.no_grad():\n",
    "            q_values_left = left_agent_model(state_left_tensor)\n",
    "            q_values_right = right_agent_model(state_right_tensor)\n",
    "\n",
    "        # Choose actions using epsilon-greedy policy\n",
    "        action_left = torch.argmax(q_values_left).item() if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "        action_right = torch.argmax(q_values_right).item() if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "        \n",
    "        # Manual human paddle movement with boundary checks\n",
    "        #keys = pygame.key.get_pressed()\n",
    "        #if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "        #    left_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "        #    left_paddle_pos[1] += 5\n",
    "        #if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "        #    right_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "        #    right_paddle_pos[1] += 5\n",
    "\n",
    "        #Left AI agent moves the paddle!!\n",
    "        if action_left == 0 and left_paddle_pos[1] > 0:  # Move Up\n",
    "            left_paddle_pos[1] -= 25\n",
    "        elif action_left == 1 and left_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            left_paddle_pos[1] += 25\n",
    "        #elif action_left == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        #Right AI agent moves the paddle!!\n",
    "        if action_right == 0 and right_paddle_pos[1] > 0:  # Move Up\n",
    "            right_paddle_pos[1] -= 25\n",
    "        elif action_right == 1 and right_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            right_paddle_pos[1] += 25\n",
    "        #elif action_right == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        # Debugging code to print current state and action for both agents\n",
    "        #print(f\"Current State Left: {state_left}, Action Taken Left: {action_left}\")\n",
    "        #print(f\"Current State Right: {state_right}, Action Taken Right: {action_right}\")\n",
    "\n",
    "        # Update ball position\n",
    "        ball_pos[0] += ball_velocity[0]\n",
    "        ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "        # Collision detection with walls\n",
    "        if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "            ball_velocity[1] = -ball_velocity[1]\n",
    "\n",
    "        # Collision detection with paddles\n",
    "        collision_offset = 5  # Define an offset to push the ball away from the paddle\n",
    "        if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "            left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] += collision_offset  # Push the ball away from the paddle\n",
    "            reward_left = reward_for_hitting_ball  # Add reward for left agent\n",
    "            contact_with_ball_made_this_episode_left = True # Note that left agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "            KR_ball_hits_left += 1 # Tracked to evaluate run of episodes\n",
    "            KR_reward_events_left += 1 # Tracked to evaluate run of episodes\n",
    "        elif (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "              right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] -= collision_offset  # Push the ball away from the paddle\n",
    "            reward_right = reward_for_hitting_ball  # Add reward for right agent\n",
    "            contact_with_ball_made_this_episode_right = True # Note that right agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "            KR_ball_hits_right += 1 # Tracked to evaluate run of episodes\n",
    "            KR_reward_events_right += 1 # Tracked to evaluate run of episodes\n",
    "\n",
    "        # Ball reset, scoring, and immediate feedback game-over condition\n",
    "        if ball_pos[0] < 0:\n",
    "            # Reset paddle positions to the middle\n",
    "            left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            #left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            right_score += 1  # Right player scores\n",
    "            KR_reward_events_right += 1 # Tracked to evaluate run of episodes\n",
    "            #Rewards\n",
    "            reward_left += punishment_for_losing_episode  # Punishment for the left agent\n",
    "            if contact_with_ball_made_this_episode_right: # Only reward if right agent made contact in this episode\n",
    "                reward_right += reward_for_winning_episode  # Positive reward for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            if this_episode_length > KR_max_episode_length: \n",
    "                KR_max_episode_length = this_episode_length\n",
    "            this_episode_length = 0 # Reset length of episode\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "        elif ball_pos[0] > width:\n",
    "            # Reset paddle positions to the middle\n",
    "            left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            #left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            left_score += 1  # Left player scores\n",
    "            KR_reward_events_left += 1 # Tracked to evaluate run of episodes\n",
    "            #Rewards\n",
    "            if contact_with_ball_made_this_episode_left: # Only reward if left agent made contact in this episode\n",
    "                reward_left += reward_for_winning_episode  # Positive reward for the left agent\n",
    "            reward_right += punishment_for_losing_episode  # Punishment for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            if this_episode_length > KR_max_episode_length: \n",
    "                KR_max_episode_length = this_episode_length\n",
    "            this_episode_length = 0 # Reset length of episode\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            \n",
    "        # After taking an action, observe new state and reward\n",
    "        new_state_left = (left_paddle_pos[1], discretize_grid(ball_pos[0], game_board_grid_size), discretize_grid(ball_pos[1], game_board_grid_size), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        new_state_right = (right_paddle_pos[1], discretize_grid(ball_pos[0], game_board_grid_size), discretize_grid(ball_pos[1], game_board_grid_size), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Check if state has changed\n",
    "        if new_state_left != state_left or new_state_right != state_right or reward_applied_this_loop:\n",
    "\n",
    "            # Calculate loss and update neural networks\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            # For left agent\n",
    "            target_left = reward_left + gamma * torch.max(left_agent_model(torch.FloatTensor([new_state_left])))\n",
    "            predicted_left = left_agent_model(state_left_tensor)[0][action_left]\n",
    "            loss_left = loss_fn(predicted_left, target_left)\n",
    "            left_optimizer.zero_grad()\n",
    "            loss_left.backward()\n",
    "            left_optimizer.step()\n",
    "\n",
    "            # For right agent\n",
    "            target_right = reward_right + gamma * torch.max(right_agent_model(torch.FloatTensor([new_state_right])))\n",
    "            predicted_right = right_agent_model(state_right_tensor)[0][action_right]\n",
    "            loss_right = loss_fn(predicted_right, target_right)\n",
    "            right_optimizer.zero_grad()\n",
    "            loss_right.backward()\n",
    "            right_optimizer.step()\n",
    "\n",
    "        # Update current state for next iteration\n",
    "        state_left = new_state_left\n",
    "        state_right = new_state_right\n",
    "\n",
    "        # Draw paddles, ball, and scores\n",
    "        window.fill((0, 0, 0))  # Clear screen\n",
    "        pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "        pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "        pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "        # Display scores\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #score_display = font.render(f\"score: {left_score} - {right_score}\", True, (255, 255, 255))\n",
    "        #window.blit(score_display, (width // 2 - 45, 10))\n",
    "\n",
    "        # Display episode count\n",
    "        font = pygame.font.SysFont(None, 30)\n",
    "        episode_display = font.render(f\"episodes played: {episode_count}\", True, (255, 255, 255))\n",
    "        window.blit(episode_display, (width // 2 - 100, 40))\n",
    "\n",
    "        pygame.display.update()\n",
    "\n",
    "        if episode_count > episodes_to_run and episodes_to_run > 0:\n",
    "            run = False\n",
    "    \n",
    "    results = {\n",
    "        'config': {\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'epsilon': epsilon,\n",
    "            'epsilon_min': epsilon_min,\n",
    "            'epsilon_decay': epsilon_decay,\n",
    "            'game_board_grid_size': game_board_grid_size,\n",
    "            'reward_for_winning_episode': reward_for_winning_episode,\n",
    "            'punishment_for_losing_episode': punishment_for_losing_episode,\n",
    "            'reward_for_hitting_ball': reward_for_hitting_ball\n",
    "        },\n",
    "        'metrics': {\n",
    "            'KR_reward_events_left': KR_reward_events_left,\n",
    "            'KR_reward_events_right': KR_reward_events_right,\n",
    "            'KR_ball_hits_left': KR_ball_hits_left,\n",
    "            'KR_ball_hits_right': KR_ball_hits_right,\n",
    "            'KR_max_episode_length': KR_max_episode_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    return results, user_quit\n",
    " \n",
    "    \n",
    "#Constants\n",
    "DATA_FILE_PREFIX = 'v8-PyTorch-'\n",
    "DEBUG_OFF = 0\n",
    "DEBUG_INFO = 1\n",
    "DEBUG_DEBUG = 2\n",
    "DEBUG_LEVEL = DEBUG_OFF # Default debug level setting\n",
    "game_board_grid_size = 50\n",
    "\n",
    "#Rewards\n",
    "reward_for_winning_episode = 1\n",
    "reward_for_hitting_ball = 1\n",
    "punishment_for_losing_episode = -1\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('AI Learns Pong')\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define ranges and step sizes for key parameters\n",
    "alpha_range = [0.5] # [0.1, 0.15, 0.2] If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. \n",
    "gamma_range = [0.9999] # [0.9, 0.95, 0.99] If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "epsilon_range = [1.0] # [0.8, 0.9, 1.0]\n",
    "epsilon_min_range = [0.1] # [0.05, 0.1, 0.15]\n",
    "epsilon_decay_range = [0.99999] # [0.995, 0.999, 0.9999] \n",
    "reward_for_winning_episode_range = [1] # [1, 2]\n",
    "punishment_for_losing_episode_range = [-5] # [-1, -2]\n",
    "reward_for_hitting_ball_range = [5] # [0.5, 1, 1.5]\n",
    "game_board_grid_size_range = [50] # [10, 50, 100]\n",
    "\n",
    "# For progress bar\n",
    "total_runs = len(alpha_range) * len(gamma_range) * len(epsilon_range) * len(epsilon_min_range) * len(epsilon_decay_range) * len(reward_for_winning_episode_range) * len(punishment_for_losing_episode_range) * len(reward_for_hitting_ball_range) * len(game_board_grid_size_range)\n",
    "completed_runs = 0\n",
    "\n",
    "# Initialize a dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Flag to check if the user wants to quit\n",
    "user_quit = False\n",
    "\n",
    "# Nested loops to iterate through each parameter combination\n",
    "for alpha in alpha_range:\n",
    "    for gamma in gamma_range:\n",
    "        for epsilon in epsilon_range:\n",
    "            for epsilon_min in epsilon_min_range:\n",
    "                for epsilon_decay in epsilon_decay_range:\n",
    "                    for reward_for_winning_episode in reward_for_winning_episode_range:\n",
    "                        for punishment_for_losing_episode in punishment_for_losing_episode_range:\n",
    "                            for reward_for_hitting_ball in reward_for_hitting_ball_range:\n",
    "                                for game_board_grid_size in game_board_grid_size_range:\n",
    "\n",
    "                                    if user_quit:\n",
    "                                        break\n",
    "\n",
    "                                    # Generate a unique identifier for this parameter combination\n",
    "                                    param_id = f\"alpha-{alpha}_gamma-{gamma}_epsilon-{epsilon}_epsilon_min-{epsilon_min}_epsilon_decay-{epsilon_decay}_reward_win-{reward_for_winning_episode}_punish_lose-{punishment_for_losing_episode}_reward_hit-{reward_for_hitting_ball}_grid_size-{game_board_grid_size}\"\n",
    "                                    logging.info(f\"Running test for parameter set: {param_id}\")\n",
    "                                    start_time = datetime.datetime.now()\n",
    "\n",
    "                                    # Run the game with the current parameter combination\n",
    "                                    episodes_to_run = 2500\n",
    "                                    results, user_quit = play_de_game(episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, game_board_grid_size, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball)\n",
    "\n",
    "                                    # Store the results in the all_results dictionary\n",
    "                                    all_results[param_id] = results\n",
    "                                    #logging.info(f\"Completed test for parameter set: {param_id}\")\n",
    "\n",
    "                                    completed_runs += 1\n",
    "                                    percent_complete = (completed_runs / total_runs) * 100\n",
    "                                    end_time = datetime.datetime.now()\n",
    "                                    time_taken = end_time - start_time\n",
    "                                    logging.info(f\"Progress: {percent_complete:.2f}% Time taken: {time_taken} for {param_id[:20]}\")\n",
    "\n",
    "                                if user_quit:\n",
    "                                    break\n",
    "                            if user_quit:\n",
    "                                break\n",
    "                        if user_quit:\n",
    "                            break\n",
    "                    if user_quit:\n",
    "                        break\n",
    "                if user_quit:\n",
    "                    break\n",
    "            if user_quit:\n",
    "                break\n",
    "        if user_quit:\n",
    "            break\n",
    "    if user_quit:\n",
    "        break\n",
    "\n",
    "\n",
    "# Save all_results to a pickle file\n",
    "#if not user_quit:\n",
    "with open(\"data/\"+DATA_FILE_PREFIX+\"ParamTest-All-Results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "    \n",
    "#print(all_results)\n",
    "\n",
    "logging.info(\"All parameter tuning runs completed.\")\n",
    "        \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76598079",
   "metadata": {},
   "source": [
    "# Parameter Tuning and Analysis for Reinforcement Learning in Pong\n",
    "\n",
    "## Refactoring the Code\n",
    "\n",
    "Initially, the Pong game was implemented in a monolithic script. To make it more modular and facilitate parameter tuning, the following steps were taken:\n",
    "\n",
    "1. **Encapsulation**: The core game logic was encapsulated into a function called `play_de_game()`.\n",
    "2. **Parameterization**: The function was designed to accept various hyperparameters as arguments, allowing for easy tuning. These parameters include:\n",
    "    - `alpha`: The learning rate\n",
    "    - `gamma`: The discount factor\n",
    "    - `epsilon`: The exploration rate\n",
    "    - `epsilon_min`: The minimum exploration rate\n",
    "    - `epsilon_decay`: The decay rate for `epsilon`\n",
    "    - `GAME_BOARD_GRID_SIZE`: The size of the game board grid\n",
    "    - `reward_for_winning_episode`: The reward for winning an episode\n",
    "    - `punishment_for_losing_episode`: The punishment for losing an episode\n",
    "    - `reward_for_hitting_ball`: The reward for hitting the ball\n",
    "\n",
    "## Running the Tests\n",
    "\n",
    "After refactoring, the game was set up to run over a thousand tests, each with 2500 episodes. The tests were designed to explore a wide range of hyperparameters:\n",
    "\n",
    "- **Alpha**: Learning rate, affecting how quickly the agent adapts to new information.\n",
    "- **Gamma**: Discount factor, influencing how much future rewards are considered.\n",
    "- **Epsilon**: Exploration rate, determining the likelihood of taking a random action.\n",
    "- **Epsilon Min**: The minimum value that `epsilon` can decay to.\n",
    "- **Epsilon Decay**: The rate at which `epsilon` decays over time.\n",
    "- **Game Board Grid Size**: Affects the complexity of the state space.\n",
    "- **Reward for Winning Episode**: Encourages the agent to win.\n",
    "- **Punishment for Losing Episode**: Discourages the agent from losing.\n",
    "- **Reward for Hitting Ball**: Encourages the agent to hit the ball.\n",
    "\n",
    "The results of each test run were stored in a Python dictionary and then serialized to a pickle file (`all_results.pkl`) for later analysis.\n",
    "\n",
    "## Data Analysis Plan\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Load Data**: Import the `all_results.pkl` file into a Pandas DataFrame.\n",
    "2. **Data Cleaning**: Remove any missing values and outliers, and convert columns to appropriate data types.\n",
    "3. **Exploratory Data Analysis (EDA)**: Use statistical and visual methods to understand the data's underlying structure.\n",
    "4. **Performance Metrics**: Evaluate the performance of different parameter sets based on metrics like average reward, episodes to convergence, etc.\n",
    "\n",
    "### Expected Insights:\n",
    "\n",
    "- **Optimal Parameters**: Identify the set of parameters that yield the best performance.\n",
    "- **Parameter Sensitivity**: Understand how sensitive the model's performance is to changes in individual parameters.\n",
    "- **Convergence Behavior**: Analyze how quickly the agent learns optimal policies under different parameter settings.\n",
    "- **Reward Dynamics**: Examine how different reward structures affect the agent's learning process.\n",
    "\n",
    "By the end of this analysis, I expect to have a comprehensive understanding of how different hyperparameters affect the learning process and performance of the reinforcement learning agent in the Pong game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4466de3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    KR_max_episode_length  game_board_grid_size  alpha  gamma   epsilon\n",
      "11                   2073                    50    0.1   0.95  0.099682\n",
      "19                    770                    50    0.2   0.95  0.099924\n",
      "9                     765                     5    0.1   0.95  0.099682\n",
      "27                    764                    50    0.2   0.95  0.099682\n",
      "0                     763                     1    0.1   0.95  0.099924\n",
      "1                     763                     5    0.1   0.95  0.099924\n",
      "3                     761                    50    0.1   0.95  0.099924\n",
      "24                    761                     1    0.2   0.95  0.099682\n",
      "8                     761                     1    0.1   0.95  0.099682\n",
      "25                    603                     5    0.2   0.95  0.099682\n",
      "18                    603                    10    0.2   0.95  0.099924\n",
      "17                    599                     5    0.2   0.95  0.099924\n",
      "16                    599                     1    0.2   0.95  0.099924\n",
      "10                    597                    10    0.1   0.95  0.099682\n",
      "7                     597                    50    0.1   0.95  0.654913\n",
      "2                     595                    10    0.1   0.95  0.099924\n",
      "26                    595                    10    0.2   0.95  0.099682\n",
      "21                    445                     5    0.2   0.95  0.654913\n",
      "22                    440                    10    0.2   0.95  0.654913\n",
      "4                     440                     1    0.1   0.95  0.654913\n",
      "6                     273                    10    0.1   0.95  0.654913\n",
      "5                     273                     5    0.1   0.95  0.654913\n",
      "20                    265                     1    0.2   0.95  0.654913\n",
      "23                    265                    50    0.2   0.95  0.654913\n",
      "30                    101                    10    0.2   0.95  0.818641\n",
      "29                    101                     5    0.2   0.95  0.818641\n",
      "28                    101                     1    0.2   0.95  0.818641\n",
      "13                    101                     5    0.1   0.95  0.818641\n",
      "14                    101                    10    0.1   0.95  0.818641\n",
      "12                    101                     1    0.1   0.95  0.818641\n",
      "15                    101                    50    0.1   0.95  0.818641\n",
      "31                    101                    50    0.2   0.95  0.818641\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open(\"data/v6-PyTorch-ParamTest-All-Results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "# Initialize empty lists to store config and metrics data\n",
    "config_data = []\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate through the all_results dictionary to separate config and metrics\n",
    "for param_id, result in all_results.items():\n",
    "    config = result['config']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Add a parameter ID to link config and metrics\n",
    "    config['param_id'] = param_id\n",
    "    metrics['param_id'] = param_id\n",
    "    \n",
    "    config_data.append(config)\n",
    "    metrics_data.append(metrics)\n",
    "\n",
    "# Convert lists of dictionaries to DataFrames\n",
    "config_df = pd.DataFrame(config_data)\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Merge the config and metrics DataFrames on 'param_id'\n",
    "df = pd.merge(config_df, metrics_df, on='param_id')\n",
    "\n",
    "# Data cleaning: Convert columns to appropriate data types, handle missing values, etc.\n",
    "# For this example, I'm assuming all columns are of the correct data type and no missing values.\n",
    "# You can add your own data cleaning steps here as needed.\n",
    "\n",
    "# Display the first few rows of the final DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "top_max_episode_length = df.sort_values(by='KR_max_episode_length', ascending=False)\n",
    "print(top_max_episode_length[['KR_max_episode_length', 'game_board_grid_size', 'alpha', 'gamma', 'epsilon']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2739a9",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462cd2",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0693c8",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as ε-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2002e88",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e387d6",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d646240",
   "metadata": {},
   "source": [
    "## Building intuition around training variables\n",
    "\n",
    "1. **Alpha (α) - Learning Rate**: \n",
    "    - **What it does**: Determines how much of the new Q-value estimate I adopt.\n",
    "    - **Intuition**: Think of it as a \"blending factor.\" If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. A value between 0 and 1 blends the old and new information.\n",
    "    - **Example**: If α is high (closer to 1), I will rapidly adapt to new strategies but may also forget useful past knowledge quickly.\n",
    "\n",
    "2. **Gamma (γ) - Discount Factor**: \n",
    "    - **What it does**: Influences how much future rewards contribute to the Q-value.\n",
    "    - **Intuition**: It's like a \"patience meter.\" A high γ makes me prioritize long-term reward over short-term reward.\n",
    "    - **Example**: If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "\n",
    "3. **Epsilon (ε) - Exploration Rate**: \n",
    "    - **What it does**: Controls the trade-off between exploration (trying new actions) and exploitation (sticking with known actions).\n",
    "    - **Intuition**: It's like the \"curiosity level.\" A high ε encourages me to try new things, while a low ε makes me stick to what I know.\n",
    "    - **Example**: If ε starts high and decays over time (ε-decay), I will initially explore a lot and gradually shift to exploiting my learned knowledge.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
