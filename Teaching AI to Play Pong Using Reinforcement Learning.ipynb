{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03b6c56",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e54589",
   "metadata": {},
   "source": [
    "# See if I can run an external Pygame window from a Jupyter notebook on macosx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecbdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create external window\n",
    "win = pygame.display.set_mode((500, 500))\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(100)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    # Game logic here (e.g., move a rectangle)\n",
    "    pygame.draw.rect(win, (255, 0, 0), (250, 250, 50, 50))\n",
    "    \n",
    "    pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae897a",
   "metadata": {},
   "source": [
    "**Well, it runs but shutdown isn't graceful.  The window pops up, draws a glorious red square.  But then simple window commands like \"close\" fail.  I had to Force Quit which then also brought the Jupyter notebook kernel to the ground.  This may wind up being a royal PITA but i'll give it a shot for now.  Worst case I'll switch to a simple python script run from the console.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09609",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "#Helper function to load data from a pickle file\n",
    "def load_data_from_pickle_file(filename, default_value):\n",
    "    try: return pickle.load(open(filename, \"rb\")) if os.path.exists(filename) else default_value\n",
    "    except Exception as e: print(f\"Error loading {filename}: {e}\"); return default_value\n",
    "\n",
    "#Convert input coordinate to discrete grid space.  this smaller grid space should make learning easier.\n",
    "def discretize_grid(coordinate): \n",
    "    return coordinate // GAME_BOARD_GRID_SIZE\n",
    "\n",
    "#Convert velocity into discretized space (of only 4 options!)\n",
    "def discretize_velocity(velocity_x, velocity_y):\n",
    "    if velocity_x > 0 and velocity_y > 0:\n",
    "        return 0  # Up-Right\n",
    "    elif velocity_x > 0 and velocity_y < 0:\n",
    "        return 1  # Down-Right\n",
    "    elif velocity_x < 0 and velocity_y > 0:\n",
    "        return 2  # Up-Left\n",
    "    elif velocity_x < 0 and velocity_y < 0:\n",
    "        return 3  # Down-Left\n",
    "    \n",
    "#Main Pong game function, accepts key parameters as inputs now\n",
    "def play_de_game(DATA_FILE_PREFIX, episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, GAME_BOARD_GRID_SIZE, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball):\n",
    "    \n",
    "    #Key Results\n",
    "    KR_reward_events_left = 0\n",
    "    KR_reward_events_right = 0\n",
    "    KR_ball_hits_left = 0\n",
    "    KR_ball_hits_right = 0\n",
    "    KR_avg_episode_length = 0\n",
    "    KR_epsilon_values = []\n",
    "    \n",
    "    # Init files\n",
    "    Q_TABLE_LEFT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_left.pkl'\n",
    "    Q_TABLE_RIGHT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_right.pkl'\n",
    "    EPISODE_COUNT_FILE = 'data/' + DATA_FILE_PREFIX + 'Episode_count.pkl'\n",
    "    \n",
    "    #Q-table save frequency\n",
    "    episode_count = 0  # Initialize episode count\n",
    "    save_frequency = 100  # Save every 100 episodes\n",
    "    \n",
    "    # Init Q-tables\n",
    "    Q_table_left = {}\n",
    "    Q_table_right = {}\n",
    "    \n",
    "    #Load data from pickle\n",
    "    Q_table_left = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "    Q_table_right = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "    episode_count = load_data_from_pickle_file(EPISODE_COUNT_FILE, 0)\n",
    "    \n",
    "    # Initialize scores\n",
    "    left_score = 0\n",
    "    right_score = 0\n",
    "    \n",
    "    # Initial paddle positions\n",
    "    #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "    #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "    # Paddle positions to a random spot\n",
    "    left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "    right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "    ball_pos = [width // 2, height // 2]\n",
    "    \n",
    "    # Initial Ball velocity\n",
    "    ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "    \n",
    "    # Initialize this_episode_length\n",
    "    episode_lengths = []\n",
    "    this_episode_length = 0\n",
    "    \n",
    "    # Init whether each AI agent has hit the ball in this episode yet\n",
    "    contact_with_ball_made_this_episode_left = False\n",
    "    contact_with_ball_made_this_episode_right = False\n",
    "    \n",
    "    # Init results\n",
    "    results = {}\n",
    "    \n",
    "    run = True\n",
    "    user_quit = False\n",
    "    while run:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "                user_quit = True\n",
    "\n",
    "        #Track game loops in this episode/game and report to screen to get a sense of how many loops a game lasts\n",
    "        this_episode_length += 1\n",
    "\n",
    "        #Debug track whether we have a rewarded event in this loop\n",
    "        reward_applied_this_loop = False\n",
    "\n",
    "        # Reset rewards to 0 at the beginning of each pass through the game loop\n",
    "        reward_left = 0\n",
    "        reward_right = 0\n",
    "\n",
    "        # Create the state representation for both agents\n",
    "        state_left = (discretize_grid(left_paddle_pos[1]), discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        state_right = (discretize_grid(right_paddle_pos[1]), discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Initialize Q-values for the states if not already present\n",
    "        if state_left not in Q_table_left:\n",
    "            Q_table_left[state_left] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "        if state_right not in Q_table_right:\n",
    "            Q_table_right[state_right] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "\n",
    "        # Choose an action for both agents using the epsilon-greedy policy\n",
    "        action_left = max(Q_table_left[state_left], key=Q_table_left[state_left].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "        action_right = max(Q_table_right[state_right], key=Q_table_right[state_right].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "\n",
    "        # Manual human paddle movement with boundary checks\n",
    "        #keys = pygame.key.get_pressed()\n",
    "        #if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "        #    left_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "        #    left_paddle_pos[1] += 5\n",
    "        #if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "        #    right_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "        #    right_paddle_pos[1] += 5\n",
    "\n",
    "        #Left AI agent moves the paddle!!\n",
    "        if action_left == 0 and left_paddle_pos[1] > 0:  # Move Up\n",
    "            left_paddle_pos[1] -= 5\n",
    "        elif action_left == 1 and left_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            left_paddle_pos[1] += 5\n",
    "        #elif action_left == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        #Right AI agent moves the paddle!!\n",
    "        if action_right == 0 and right_paddle_pos[1] > 0:  # Move Up\n",
    "            right_paddle_pos[1] -= 5\n",
    "        elif action_right == 1 and right_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            right_paddle_pos[1] += 5\n",
    "        #elif action_right == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        # Debugging code to print current state and action for both agents\n",
    "        #print(f\"Current State Left: {state_left}, Action Taken Left: {action_left}\")\n",
    "        #print(f\"Current State Right: {state_right}, Action Taken Right: {action_right}\")\n",
    "\n",
    "        # Update ball position\n",
    "        ball_pos[0] += ball_velocity[0]\n",
    "        ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "        # Collision detection with walls\n",
    "        if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "            ball_velocity[1] = -ball_velocity[1]\n",
    "\n",
    "        # Collision detection with paddles\n",
    "        collision_offset = 5  # Define an offset to push the ball away from the paddle\n",
    "        if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "            left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] += collision_offset  # Push the ball away from the paddle\n",
    "            reward_left = reward_for_hitting_ball  # Add reward for left agent\n",
    "            contact_with_ball_made_this_episode_left = True # Note that left agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "            KR_ball_hits_left += 1 # Tracked to evaluate run of episodes\n",
    "            KR_reward_events_left += 1 # Tracked to evaluate run of episodes\n",
    "        elif (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "              right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] -= collision_offset  # Push the ball away from the paddle\n",
    "            reward_right = reward_for_hitting_ball  # Add reward for right agent\n",
    "            contact_with_ball_made_this_episode_right = True # Note that right agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "            KR_ball_hits_right += 1 # Tracked to evaluate run of episodes\n",
    "            KR_reward_events_right += 1 # Tracked to evaluate run of episodes\n",
    "\n",
    "        #Penalties for not exploring enough\n",
    "        #extreme_zones = [[0, height // 8], [7 * height // 8, height]]  # Define the extreme zones\n",
    "        #bonus = 0.1  # Define the bonus\n",
    "        #center_zone = [height // 4, 3 * height // 4]  # Define the center zone\n",
    "        #penalty = -0.1  # Define the penalty\n",
    "        # Apply penalty for center zone\n",
    "        #if center_zone[0] <= left_paddle_pos[1] <= center_zone[1]:\n",
    "        #    reward_left += penalty\n",
    "        #if center_zone[0] <= right_paddle_pos[1] <= center_zone[1]:\n",
    "        #    reward_right += penalty\n",
    "        # Apply bonus for extreme zones\n",
    "        #for zone in extreme_zones:\n",
    "        #    if zone[0] <= left_paddle_pos[1] <= zone[1]:\n",
    "        #        reward_left += bonus\n",
    "        #    if zone[0] <= right_paddle_pos[1] <= zone[1]:\n",
    "        #        reward_right += bonus\n",
    "\n",
    "        # Ball reset, scoring, and immediate feedback game-over condition\n",
    "        if ball_pos[0] < 0:\n",
    "            # Reset paddle positions to the middle\n",
    "            #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            right_score += 1  # Right player scores\n",
    "            KR_reward_events_right += 1 # Tracked to evaluate run of episodes\n",
    "            #Rewards\n",
    "            reward_left += punishment_for_losing_episode  # Punishment for the left agent\n",
    "            if contact_with_ball_made_this_episode_right: # Only reward if right agent made contact in this episode\n",
    "                reward_right += reward_for_winning_episode  # Positive reward for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            # Keep track of average episode length\n",
    "            episode_lengths.append(this_episode_length)\n",
    "            KR_avg_episode_length = sum(episode_lengths) / len(episode_lengths)\n",
    "            this_episode_length = 0 # Reset length of episode\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            #KR_epsilon_values.append(epsilon) # Tracked to evaluate run of episodes\n",
    "            # Save the Q-tables every save_frequency episodes\n",
    "            #if episode_count % save_frequency == 0:\n",
    "            #    with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(Q_table_left, f)\n",
    "            #    with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(Q_table_right, f)\n",
    "            #    with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(episode_count, f)\n",
    "        elif ball_pos[0] > width:\n",
    "            # Reset paddle positions to the middle\n",
    "            #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            left_score += 1  # Left player scores\n",
    "            KR_reward_events_left += 1 # Tracked to evaluate run of episodes\n",
    "            #Rewards\n",
    "            if contact_with_ball_made_this_episode_left: # Only reward if left agent made contact in this episode\n",
    "                reward_left += reward_for_winning_episode  # Positive reward for the left agent\n",
    "            reward_right += punishment_for_losing_episode  # Punishment for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            # Keep track of average episode length\n",
    "            episode_lengths.append(this_episode_length)\n",
    "            KR_avg_episode_length = sum(episode_lengths) / len(episode_lengths)\n",
    "            this_episode_length = 0 # Reset length of episode\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            #KR_epsilon_values.append(epsilon) # Tracked to evaluate run of episodes\n",
    "            # Save the Q-tables every save_frequency episodes\n",
    "            #if episode_count % save_frequency == 0:\n",
    "            #    with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(Q_table_left, f)\n",
    "            #    with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(Q_table_right, f)\n",
    "            #    with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "            #        pickle.dump(episode_count, f)\n",
    "\n",
    "        # After taking an action, observe new state and reward\n",
    "        new_state_left = (discretize_grid(left_paddle_pos[1]), right_paddle_pos[1], discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        new_state_right = (discretize_grid(left_paddle_pos[1]), right_paddle_pos[1], discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Check if state has changed\n",
    "        if new_state_left != state_left or new_state_right != state_right or reward_applied_this_loop:\n",
    "\n",
    "            # Initialize Q-values-left for the new state if not already present\n",
    "            if new_state_left not in Q_table_left:\n",
    "                Q_table_left[new_state_left] = {action: 0 for action in action_space}\n",
    "\n",
    "            # Initialize Q-values-right for the new state if not already present\n",
    "            if new_state_right not in Q_table_right:\n",
    "                Q_table_right[new_state_right] = {action: 0 for action in action_space}\n",
    "\n",
    "            # Calculate the best next action for both agents\n",
    "            best_next_action_left = max(Q_table_left[new_state_left], key=Q_table_left[new_state_left].get)\n",
    "            best_next_action_right = max(Q_table_right[new_state_right], key=Q_table_right[new_state_right].get)\n",
    "\n",
    "            # Q-Learning update rule for both agents\n",
    "            Q_table_left[state_left][action_left] = (1 - alpha) * Q_table_left[state_left][action_left] + alpha * (reward_left + gamma * Q_table_left[new_state_left][best_next_action_left])\n",
    "            Q_table_right[state_right][action_right] = (1 - alpha) * Q_table_right[state_right][action_right] + alpha * (reward_right + gamma * Q_table_right[new_state_right][best_next_action_right])\n",
    "\n",
    "        # Update current state for next iteration\n",
    "        state_left = new_state_left\n",
    "        state_right = new_state_right\n",
    "\n",
    "        # Draw paddles, ball, and scores\n",
    "        #window.fill((0, 0, 0))  # Clear screen\n",
    "        #pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "        #pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "        #pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "        # Display scores\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #score_display = font.render(f\"score: {left_score} - {right_score}\", True, (255, 255, 255))\n",
    "        #window.blit(score_display, (width // 2 - 45, 10))\n",
    "\n",
    "        # Display episode count\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #episode_display = font.render(f\"episodes played: {episode_count}\", True, (255, 255, 255))\n",
    "        #window.blit(episode_display, (width // 2 - 100, 40))\n",
    "\n",
    "        # Display current epsilon\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #epsilon_display = font.render(f\"Epsilon: {epsilon:.4f}\", True, (255, 255, 255))\n",
    "        #window.blit(epsilon_display, (10, 70))\n",
    "\n",
    "        # Display current frame within game\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #epsilon_display = font.render(f\"this_episode_length: {this_episode_length}\", True, (255, 255, 255))\n",
    "        #window.blit(epsilon_display, (10, 160))\n",
    "\n",
    "        #pygame.display.update()\n",
    "\n",
    "        if episode_count > episodes_to_run and episodes_to_run > 0:\n",
    "            run = False\n",
    "    \n",
    "    results = {\n",
    "        'config': {\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'epsilon': epsilon,\n",
    "            'epsilon_min': epsilon_min,\n",
    "            'epsilon_decay': epsilon_decay,\n",
    "            'GAME_BOARD_GRID_SIZE': GAME_BOARD_GRID_SIZE,\n",
    "            'reward_for_winning_episode': reward_for_winning_episode,\n",
    "            'punishment_for_losing_episode': punishment_for_losing_episode,\n",
    "            'reward_for_hitting_ball': reward_for_hitting_ball\n",
    "        },\n",
    "        'metrics': {\n",
    "            'KR_reward_events_left': KR_reward_events_left,\n",
    "            'KR_reward_events_right': KR_reward_events_right,\n",
    "            'KR_ball_hits_left': KR_ball_hits_left,\n",
    "            'KR_ball_hits_right': KR_ball_hits_right,\n",
    "            'KR_avg_episode_length': KR_avg_episode_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, user_quit\n",
    " \n",
    "    \n",
    "#Constants\n",
    "DATA_FILE_PREFIX = 'v1-'\n",
    "DEBUG_OFF = 0\n",
    "DEBUG_INFO = 1\n",
    "DEBUG_DEBUG = 2\n",
    "DEBUG_LEVEL = DEBUG_OFF # Default debug level setting\n",
    "GAME_BOARD_GRID_SIZE = 50\n",
    "\n",
    "#Rewards\n",
    "reward_for_winning_episode = 1\n",
    "reward_for_hitting_ball = 1\n",
    "punishment_for_losing_episode = -1\n",
    "\n",
    "# Initialize epsilon for the epsilon-greedy policy\n",
    "epsilon = 1.0 #(orig 1.0)\n",
    "epsilon_min = 0.10 #(orig .01)\n",
    "epsilon_decay = 0.999995 #(orig .995)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "alpha = 0.15  # Learning rate (orig .1)\n",
    "gamma = 0.95  # Discount factor (orig .99)\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('AI Learns Pong')\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define ranges and step sizes for key parameters\n",
    "alpha_range = [0.1, 0.15, 0.2] # [0.1, 0.15, 0.2]\n",
    "gamma_range = [0.9, 0.95, 0.99] # [0.9, 0.95, 0.99]\n",
    "epsilon_range = [0.8, 0.9, 1.0] # [0.8, 0.9, 1.0]\n",
    "epsilon_min_range = [0.05, 0.1, 0.15] # [0.05, 0.1, 0.15]\n",
    "epsilon_decay_range = [0.995, 0.999, 0.9999] # [0.995, 0.999, 0.9999] \n",
    "reward_for_winning_episode_range = [1, 2] # [1, 2]\n",
    "punishment_for_losing_episode_range = [-1, -2] # [-1, -2]\n",
    "reward_for_hitting_ball_range = [0.5, 1, 1.5, 2.0] # [0.5, 1, 1.5]\n",
    "GAME_BOARD_GRID_SIZE_range = [50, 100]\n",
    "\n",
    "# For progress bar\n",
    "total_runs = len(alpha_range) * len(gamma_range) * len(epsilon_range) * len(epsilon_min_range) * len(epsilon_decay_range) * len(reward_for_winning_episode_range) * len(punishment_for_losing_episode_range) * len(reward_for_hitting_ball_range) * len(GAME_BOARD_GRID_SIZE_range)\n",
    "completed_runs = 0\n",
    "\n",
    "# Initialize a dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Flag to check if the user wants to quit\n",
    "user_quit = False\n",
    "\n",
    "# Nested loops to iterate through each parameter combination\n",
    "for alpha in alpha_range:\n",
    "    for gamma in gamma_range:\n",
    "        for epsilon in epsilon_range:\n",
    "            for epsilon_min in epsilon_min_range:\n",
    "                for epsilon_decay in epsilon_decay_range:\n",
    "                    for reward_for_winning_episode in reward_for_winning_episode_range:\n",
    "                        for punishment_for_losing_episode in punishment_for_losing_episode_range:\n",
    "                            for reward_for_hitting_ball in reward_for_hitting_ball_range:\n",
    "                                for GAME_BOARD_GRID_SIZE in GAME_BOARD_GRID_SIZE_range:\n",
    "                                    \n",
    "                                    if user_quit:\n",
    "                                        break\n",
    "\n",
    "                                    # Generate a unique identifier for this parameter combination\n",
    "                                    param_id = f\"alpha-{alpha}_gamma-{gamma}_epsilon-{epsilon}_epsilon_min-{epsilon_min}_epsilon_decay-{epsilon_decay}_reward_win-{reward_for_winning_episode}_punish_lose-{punishment_for_losing_episode}_reward_hit-{reward_for_hitting_ball}\"\n",
    "\n",
    "                                    #logging.info(f\"Running test for parameter set: {param_id}\")\n",
    "                                    \n",
    "                                    start_time = datetime.datetime.now()\n",
    "\n",
    "                                    # Run the game with the current parameter combination\n",
    "                                    DATA_FILE_PREFIX_WITH_PARAMS = f\"ParamTest-{DATA_FILE_PREFIX}{param_id}-\"\n",
    "                                    episodes_to_run = 2000\n",
    "                                    results, user_quit = play_de_game(DATA_FILE_PREFIX_WITH_PARAMS, episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, GAME_BOARD_GRID_SIZE, reward_for_winning_episode, punishment_for_losing_episode, reward_for_hitting_ball)\n",
    "\n",
    "                                    # Store the results in the all_results dictionary\n",
    "                                    all_results[param_id] = results\n",
    "                                    #logging.info(f\"Completed test for parameter set: {param_id}\")\n",
    "                                    \n",
    "                                    completed_runs += 1\n",
    "                                    percent_complete = (completed_runs / total_runs) * 100\n",
    "                                    end_time = datetime.datetime.now()\n",
    "                                    time_taken = end_time - start_time\n",
    "                                    logging.info(f\"Progress: {percent_complete:.2f}% Time taken: {time_taken} for {param_id[:20]}\")\n",
    "\n",
    "                                if user_quit:\n",
    "                                    break\n",
    "                            if user_quit:\n",
    "                                break\n",
    "                        if user_quit:\n",
    "                            break\n",
    "                    if user_quit:\n",
    "                        break\n",
    "                if user_quit:\n",
    "                    break\n",
    "            if user_quit:\n",
    "                break\n",
    "        if user_quit:\n",
    "            break\n",
    "    if user_quit:\n",
    "        break\n",
    "\n",
    "# Save all_results to a pickle file\n",
    "if not user_quit:\n",
    "    with open(\"data/\"+DATA_FILE_PREFIX+\"ParamTest-All-Results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_results, f)\n",
    "\n",
    "logging.info(\"All parameter tuning runs completed.\")\n",
    "        \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf75aef",
   "metadata": {},
   "source": [
    "# Parameter Tuning and Analysis for Reinforcement Learning in Pong\n",
    "\n",
    "## Refactoring the Code\n",
    "\n",
    "Initially, the Pong game was implemented in a monolithic script. To make it more modular and facilitate parameter tuning, the following steps were taken:\n",
    "\n",
    "1. **Encapsulation**: The core game logic was encapsulated into a function called `play_de_game()`.\n",
    "2. **Parameterization**: The function was designed to accept various hyperparameters as arguments, allowing for easy tuning. These parameters include:\n",
    "    - `alpha`: The learning rate\n",
    "    - `gamma`: The discount factor\n",
    "    - `epsilon`: The exploration rate\n",
    "    - `epsilon_min`: The minimum exploration rate\n",
    "    - `epsilon_decay`: The decay rate for `epsilon`\n",
    "    - `GAME_BOARD_GRID_SIZE`: The size of the game board grid\n",
    "    - `reward_for_winning_episode`: The reward for winning an episode\n",
    "    - `punishment_for_losing_episode`: The punishment for losing an episode\n",
    "    - `reward_for_hitting_ball`: The reward for hitting the ball\n",
    "\n",
    "## Running the Tests\n",
    "\n",
    "After refactoring, the game was set up to run over a thousand tests, each with 2500 episodes. The tests were designed to explore a wide range of hyperparameters:\n",
    "\n",
    "- **Alpha**: Learning rate, affecting how quickly the agent adapts to new information.\n",
    "- **Gamma**: Discount factor, influencing how much future rewards are considered.\n",
    "- **Epsilon**: Exploration rate, determining the likelihood of taking a random action.\n",
    "- **Epsilon Min**: The minimum value that `epsilon` can decay to.\n",
    "- **Epsilon Decay**: The rate at which `epsilon` decays over time.\n",
    "- **Game Board Grid Size**: Affects the complexity of the state space.\n",
    "- **Reward for Winning Episode**: Encourages the agent to win.\n",
    "- **Punishment for Losing Episode**: Discourages the agent from losing.\n",
    "- **Reward for Hitting Ball**: Encourages the agent to hit the ball.\n",
    "\n",
    "The results of each test run were stored in a Python dictionary and then serialized to a pickle file (`all_results.pkl`) for later analysis.\n",
    "\n",
    "## Data Analysis Plan\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Load Data**: Import the `all_results.pkl` file into a Pandas DataFrame.\n",
    "2. **Data Cleaning**: Remove any missing values and outliers, and convert columns to appropriate data types.\n",
    "3. **Exploratory Data Analysis (EDA)**: Use statistical and visual methods to understand the data's underlying structure.\n",
    "4. **Performance Metrics**: Evaluate the performance of different parameter sets based on metrics like average reward, episodes to convergence, etc.\n",
    "\n",
    "### Expected Insights:\n",
    "\n",
    "- **Optimal Parameters**: Identify the set of parameters that yield the best performance.\n",
    "- **Parameter Sensitivity**: Understand how sensitive the model's performance is to changes in individual parameters.\n",
    "- **Convergence Behavior**: Analyze how quickly the agent learns optimal policies under different parameter settings.\n",
    "- **Reward Dynamics**: Examine how different reward structures affect the agent's learning process.\n",
    "\n",
    "By the end of this analysis, I expect to have a comprehensive understanding of how different hyperparameters affect the learning process and performance of the reinforcement learning agent in the Pong game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open(\"data/v01-ParamTest-All-Results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "# Initialize empty lists to store config and metrics data\n",
    "config_data = []\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate through the all_results dictionary to separate config and metrics\n",
    "for param_id, result in all_results.items():\n",
    "    config = result['config']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Add a parameter ID to link config and metrics\n",
    "    config['param_id'] = param_id\n",
    "    metrics['param_id'] = param_id\n",
    "    \n",
    "    config_data.append(config)\n",
    "    metrics_data.append(metrics)\n",
    "\n",
    "# Convert lists of dictionaries to DataFrames\n",
    "config_df = pd.DataFrame(config_data)\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Merge the config and metrics DataFrames on 'param_id'\n",
    "final_df = pd.merge(config_df, metrics_df, on='param_id')\n",
    "\n",
    "# Data cleaning: Convert columns to appropriate data types, handle missing values, etc.\n",
    "# For this example, I'm assuming all columns are of the correct data type and no missing values.\n",
    "# You can add your own data cleaning steps here as needed.\n",
    "\n",
    "# Display the first few rows of the final DataFrame\n",
    "print(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2739a9",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462cd2",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0693c8",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as ε-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2002e88",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e387d6",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d646240",
   "metadata": {},
   "source": [
    "## Building intuition around training variables\n",
    "\n",
    "1. **Alpha (α) - Learning Rate**: \n",
    "    - **What it does**: Determines how much of the new Q-value estimate I adopt.\n",
    "    - **Intuition**: Think of it as a \"blending factor.\" If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. A value between 0 and 1 blends the old and new information.\n",
    "    - **Example**: If α is high (closer to 1), I will rapidly adapt to new strategies but may also forget useful past knowledge quickly.\n",
    "\n",
    "2. **Gamma (γ) - Discount Factor**: \n",
    "    - **What it does**: Influences how much future rewards contribute to the Q-value.\n",
    "    - **Intuition**: It's like a \"patience meter.\" A high γ makes me prioritize long-term reward over short-term reward.\n",
    "    - **Example**: If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "\n",
    "3. **Epsilon (ε) - Exploration Rate**: \n",
    "    - **What it does**: Controls the trade-off between exploration (trying new actions) and exploitation (sticking with known actions).\n",
    "    - **Intuition**: It's like the \"curiosity level.\" A high ε encourages me to try new things, while a low ε makes me stick to what I know.\n",
    "    - **Example**: If ε starts high and decays over time (ε-decay), I will initially explore a lot and gradually shift to exploiting my learned knowledge.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
