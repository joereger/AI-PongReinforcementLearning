{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7ce0a6",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb1e3d",
   "metadata": {},
   "source": [
    "# See if I can run an external Pygame window from a Jupyter notebook on macosx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f53e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create external window\n",
    "win = pygame.display.set_mode((500, 500))\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(100)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    # Game logic here (e.g., move a rectangle)\n",
    "    pygame.draw.rect(win, (255, 0, 0), (250, 250, 50, 50))\n",
    "    \n",
    "    pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24da492",
   "metadata": {},
   "source": [
    "**Well, it runs but shutdown isn't graceful.  The window pops up, draws a glorious red square.  But then simple window commands like \"close\" fail.  I had to Force Quit which then also brought the Jupyter notebook kernel to the ground.  This may wind up being a royal PITA but i'll give it a shot for now.  Worst case I'll switch to a simple python script run from the console.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98297e9",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080af3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('Pong Game')\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "# Initial positions\n",
    "left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "ball_pos = [width // 2, height // 2]\n",
    "\n",
    "# Ball velocity\n",
    "ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "\n",
    "# Initialize scores\n",
    "left_score = 0\n",
    "right_score = 0\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize reward\n",
    "reward = 0\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = {}\n",
    "\n",
    "# Initialize epsilon for the epsilon-greedy policy\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(30)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "\n",
    "    # Create the state representation\n",
    "    state = (left_paddle_pos[1], right_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "\n",
    "    # Initialize Q-values for the state if not already present\n",
    "    if state not in Q_table:\n",
    "        Q_table[state] = {action: 0 for action in action_space}\n",
    "\n",
    "    # Choose an action using the epsilon-greedy policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(action_space)\n",
    "    else:\n",
    "        action = max(Q_table[state], key=Q_table[state].get)\n",
    "\n",
    "    # Choose an action using a policy (to be implemented)\n",
    "    # ...\n",
    "    \n",
    "    # Handle paddle movement with boundary checks\n",
    "    keys = pygame.key.get_pressed()\n",
    "    if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "        left_paddle_pos[1] -= 5\n",
    "    if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "        left_paddle_pos[1] += 5\n",
    "    if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "        right_paddle_pos[1] -= 5\n",
    "    if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "        right_paddle_pos[1] += 5\n",
    "\n",
    "    # Update ball position\n",
    "    ball_pos[0] += ball_velocity[0]\n",
    "    ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "    # Collision detection with walls\n",
    "    if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "        ball_velocity[1] = -ball_velocity[1]\n",
    "\n",
    "    # Collision detection with paddles\n",
    "    if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "        left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height) or \\\n",
    "       (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "        right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "        ball_velocity[0] = -ball_velocity[0]\n",
    "        \n",
    "    # After taking an action, observe new state and reward\n",
    "    new_state = (left_paddle_pos[1], right_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "    reward = 0  # Initialize to 0; this will be updated based on game events\n",
    "\n",
    "    # Ball reset, scoring, and immediate feedback game-over condition\n",
    "    if ball_pos[0] < 0:\n",
    "        right_score += 1  # Right player scores\n",
    "        reward = -1  # Negative reward for the agent\n",
    "        # Here, i'll signal the end of an RL episode and update the agent\n",
    "    elif ball_pos[0] > width:\n",
    "        left_score += 1  # Left player scores\n",
    "        reward = 1  # Positive reward for the agent\n",
    "        # here's where i'll signal the the end of an RL episode and update the agent\n",
    "        \n",
    "    # Initialize Q-values for the new state if not already present\n",
    "    if new_state not in Q_table:\n",
    "        Q_table[new_state] = {action: 0 for action in action_space}\n",
    "\n",
    "    # Q-Learning update rule\n",
    "    best_next_action = max(Q_table[new_state], key=Q_table[new_state].get)  # Best action in new state\n",
    "    Q_table[state][action] = (1 - alpha) * Q_table[state][action] + alpha * (reward + gamma * Q_table[new_state][best_next_action])\n",
    "\n",
    "    # Update current state for next iteration\n",
    "    state = new_state\n",
    "\n",
    "    # Draw paddles, ball, and scores\n",
    "    window.fill((0, 0, 0))  # Clear screen\n",
    "    pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "    pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "    pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "    # Display scores\n",
    "    font = pygame.font.SysFont(None, 36)\n",
    "    score_display = font.render(f\"{left_score} - {right_score}\", True, (255, 255, 255))\n",
    "    window.blit(score_display, (width // 2 - 20, 10))\n",
    "\n",
    "    pygame.display.update()\n",
    "    \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1a379",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f3ad0",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349b90c",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as Îµ-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32f747",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a1ab2",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd76b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
