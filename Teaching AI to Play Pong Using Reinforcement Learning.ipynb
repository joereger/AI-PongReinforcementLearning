{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03b6c56",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e54589",
   "metadata": {},
   "source": [
    "# See if I can run an external Pygame window from a Jupyter notebook on macosx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecbdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create external window\n",
    "win = pygame.display.set_mode((500, 500))\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(100)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    # Game logic here (e.g., move a rectangle)\n",
    "    pygame.draw.rect(win, (255, 0, 0), (250, 250, 50, 50))\n",
    "    \n",
    "    pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae897a",
   "metadata": {},
   "source": [
    "**Well, it runs but shutdown isn't graceful.  The window pops up, draws a glorious red square.  But then simple window commands like \"close\" fail.  I had to Force Quit which then also brought the Jupyter notebook kernel to the ground.  This may wind up being a royal PITA but i'll give it a shot for now.  Worst case I'll switch to a simple python script run from the console.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09609",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8d21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "results:  {'config': {'alpha': 0.15, 'gamma': 0.95, 'epsilon': 0.9980618757430635, 'epsilon_min': 0.1, 'epsilon_decay': 0.999995, 'GAME_BOARD_GRID_SIZE': 50, 'reward_for_winning_episode': 1, 'reward_for_hitting_ball': 1}, 'metrics': {'KR_reward_events_left': 0, 'KR_reward_events_right': 0, 'KR_ball_hits_left': 0, 'KR_ball_hits_right': 0, 'KR_win_loss_ratio': 0, 'KR_avg_episode_length': 0, 'KR_epsilon_values': []}}\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "#Helper function to load data from a pickle file\n",
    "def load_data_from_pickle_file(filename, default_value):\n",
    "    try: return pickle.load(open(filename, \"rb\")) if os.path.exists(filename) else default_value\n",
    "    except Exception as e: print(f\"Error loading {filename}: {e}\"); return default_value\n",
    "\n",
    "#Convert input coordinate to discrete grid space.  this smaller grid space should make learning easier.\n",
    "def discretize_grid(coordinate): \n",
    "    return coordinate // GAME_BOARD_GRID_SIZE\n",
    "\n",
    "#Convert velocity into discretized space (of only 4 options!)\n",
    "def discretize_velocity(velocity_x, velocity_y):\n",
    "    if velocity_x > 0 and velocity_y > 0:\n",
    "        return 0  # Up-Right\n",
    "    elif velocity_x > 0 and velocity_y < 0:\n",
    "        return 1  # Down-Right\n",
    "    elif velocity_x < 0 and velocity_y > 0:\n",
    "        return 2  # Up-Left\n",
    "    elif velocity_x < 0 and velocity_y < 0:\n",
    "        return 3  # Down-Left\n",
    "    \n",
    "#Main Pong game function, accepts key parameters as inputs now\n",
    "def play_de_game(DATA_FILE_PREFIX, episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, GAME_BOARD_GRID_SIZE, reward_for_winning_episode, reward_for_hitting_ball):\n",
    "    \n",
    "    #Key Results\n",
    "    KR_reward_events_left = 0\n",
    "    KR_reward_events_right = 0\n",
    "    KR_ball_hits_left = 0\n",
    "    KR_ball_hits_right = 0\n",
    "    KR_win_loss_ratio = 0\n",
    "    KR_avg_episode_length = 0\n",
    "    KR_epsilon_values = []\n",
    "    \n",
    "    # Init files\n",
    "    Q_TABLE_LEFT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_left.pkl'\n",
    "    Q_TABLE_RIGHT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_right.pkl'\n",
    "    EPISODE_COUNT_FILE = 'data/' + DATA_FILE_PREFIX + 'Episode_count.pkl'\n",
    "    \n",
    "    #Q-table save frequency\n",
    "    episode_count = 0  # Initialize episode count\n",
    "    save_frequency = 100  # Save every 100 episodes\n",
    "    \n",
    "    # Init Q-tables\n",
    "    Q_table_left = {}\n",
    "    Q_table_right = {}\n",
    "    \n",
    "    #Load data from pickle\n",
    "    Q_table_left = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "    Q_table_right = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "    episode_count = load_data_from_pickle_file(EPISODE_COUNT_FILE, 0)\n",
    "    \n",
    "    # Initialize scores\n",
    "    left_score = 0\n",
    "    right_score = 0\n",
    "    \n",
    "    # Initial paddle positions\n",
    "    #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "    #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "    # Paddle positions to a random spot\n",
    "    left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "    right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "    ball_pos = [width // 2, height // 2]\n",
    "    \n",
    "    # Initial Ball velocity\n",
    "    ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "    \n",
    "    # Initialize frames_so_far_this_episode\n",
    "    frames_so_far_this_episode = 0\n",
    "    \n",
    "    # Init whether each AI agent has hit the ball in this episode yet\n",
    "    contact_with_ball_made_this_episode_left = False\n",
    "    contact_with_ball_made_this_episode_right = False\n",
    "    \n",
    "    # Init results\n",
    "    results = {}\n",
    "    \n",
    "    run = True\n",
    "    while run:\n",
    "        #pygame.time.delay(10)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "        #Track game loops in this episode/game and report to screen to get a sense of how many loops a game lasts\n",
    "        frames_so_far_this_episode += 1\n",
    "\n",
    "        #Debug track whether we have a rewarded event in this loop\n",
    "        reward_applied_this_loop = False\n",
    "\n",
    "        # Reset rewards to 0 at the beginning of each pass through the game loop\n",
    "        reward_left = 0\n",
    "        reward_right = 0\n",
    "\n",
    "        # Create the state representation for both agents\n",
    "        state_left = (discretize_grid(left_paddle_pos[1]), discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        state_right = (discretize_grid(right_paddle_pos[1]), discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Initialize Q-values for the states if not already present\n",
    "        if state_left not in Q_table_left:\n",
    "            Q_table_left[state_left] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "        if state_right not in Q_table_right:\n",
    "            Q_table_right[state_right] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "\n",
    "        # Choose an action for both agents using the epsilon-greedy policy\n",
    "        action_left = max(Q_table_left[state_left], key=Q_table_left[state_left].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "        action_right = max(Q_table_right[state_right], key=Q_table_right[state_right].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "\n",
    "        # Manual human paddle movement with boundary checks\n",
    "        #keys = pygame.key.get_pressed()\n",
    "        #if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "        #    left_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "        #    left_paddle_pos[1] += 5\n",
    "        #if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "        #    right_paddle_pos[1] -= 5\n",
    "        #if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "        #    right_paddle_pos[1] += 5\n",
    "\n",
    "        #Left AI agent moves the paddle!!\n",
    "        if action_left == 0 and left_paddle_pos[1] > 0:  # Move Up\n",
    "            left_paddle_pos[1] -= 5\n",
    "        elif action_left == 1 and left_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            left_paddle_pos[1] += 5\n",
    "        #elif action_left == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        #Right AI agent moves the paddle!!\n",
    "        if action_right == 0 and right_paddle_pos[1] > 0:  # Move Up\n",
    "            right_paddle_pos[1] -= 5\n",
    "        elif action_right == 1 and right_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "            right_paddle_pos[1] += 5\n",
    "        #elif action_right == 2: \n",
    "            # Stay Still, so no movement\n",
    "\n",
    "        # Debugging code to print current state and action for both agents\n",
    "        #print(f\"Current State Left: {state_left}, Action Taken Left: {action_left}\")\n",
    "        #print(f\"Current State Right: {state_right}, Action Taken Right: {action_right}\")\n",
    "\n",
    "        # Update ball position\n",
    "        ball_pos[0] += ball_velocity[0]\n",
    "        ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "        # Collision detection with walls\n",
    "        if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "            ball_velocity[1] = -ball_velocity[1]\n",
    "\n",
    "        # Collision detection with paddles\n",
    "        collision_offset = 5  # Define an offset to push the ball away from the paddle\n",
    "        if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "            left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] += collision_offset  # Push the ball away from the paddle\n",
    "            reward_left = reward_for_hitting_ball  # Add reward for left agent\n",
    "            contact_with_ball_made_this_episode_left = True # Note that left agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "        elif (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "              right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "            ball_velocity[0] = -ball_velocity[0]\n",
    "            ball_pos[0] -= collision_offset  # Push the ball away from the paddle\n",
    "            reward_right = reward_for_hitting_ball  # Add reward for right agent\n",
    "            contact_with_ball_made_this_episode_right = True # Note that right agent has contacted ball this episode\n",
    "            reward_applied_this_loop = True\n",
    "\n",
    "        #Penalties for not exploring enough\n",
    "        #extreme_zones = [[0, height // 8], [7 * height // 8, height]]  # Define the extreme zones\n",
    "        #bonus = 0.1  # Define the bonus\n",
    "        #center_zone = [height // 4, 3 * height // 4]  # Define the center zone\n",
    "        #penalty = -0.1  # Define the penalty\n",
    "        # Apply penalty for center zone\n",
    "        #if center_zone[0] <= left_paddle_pos[1] <= center_zone[1]:\n",
    "        #    reward_left += penalty\n",
    "        #if center_zone[0] <= right_paddle_pos[1] <= center_zone[1]:\n",
    "        #    reward_right += penalty\n",
    "        # Apply bonus for extreme zones\n",
    "        #for zone in extreme_zones:\n",
    "        #    if zone[0] <= left_paddle_pos[1] <= zone[1]:\n",
    "        #        reward_left += bonus\n",
    "        #    if zone[0] <= right_paddle_pos[1] <= zone[1]:\n",
    "        #        reward_right += bonus\n",
    "\n",
    "        # Ball reset, scoring, and immediate feedback game-over condition\n",
    "        if ball_pos[0] < 0:\n",
    "            # Reset paddle positions to the middle\n",
    "            #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            right_score += 1  # Right player scores\n",
    "            #Rewards\n",
    "            reward_left += -1 * reward_for_winning_episode  # Negative reward for the left agent\n",
    "            if contact_with_ball_made_this_episode_right: # Only reward if right agent made contact in this episode\n",
    "                reward_right += reward_for_winning_episode  # Positive reward for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            iterations_this_game = 0\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            # Save the Q-tables every save_frequency episodes\n",
    "            if episode_count % save_frequency == 0:\n",
    "                with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(Q_table_left, f)\n",
    "                with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(Q_table_right, f)\n",
    "                with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(episode_count, f)\n",
    "        elif ball_pos[0] > width:\n",
    "            # Reset paddle positions to the middle\n",
    "            #left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "            #right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "            # Reset paddle positions to a random spot\n",
    "            left_paddle_pos = [50, random.randint(0, height - paddle_height)]\n",
    "            right_paddle_pos = [width - 50 - paddle_width, random.randint(0, height - paddle_height)]\n",
    "            #Reset the ball to the center in a random direction\n",
    "            ball_pos = [width // 2, height // 2]\n",
    "            ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "            #Scoring\n",
    "            left_score += 1  # Left player scores\n",
    "            #Rewards\n",
    "            if contact_with_ball_made_this_episode_left: # Only reward if left agent made contact in this episode\n",
    "                reward_left += reward_for_winning_episode  # Positive reward for the left agent\n",
    "            reward_right += -1 * reward_for_winning_episode  # Negative reward for the right agent\n",
    "            reward_applied_this_loop = True\n",
    "            contact_with_ball_made_this_episode_left = False # Reset\n",
    "            contact_with_ball_made_this_episode_right = False # Reset\n",
    "            #Signal the end of an episode\n",
    "            episode_count += 1  # Increment episode count\n",
    "            iterations_this_game = 0\n",
    "            # Decay epsilon at the end of a game/episode\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            # Save the Q-tables every save_frequency episodes\n",
    "            if episode_count % save_frequency == 0:\n",
    "                with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(Q_table_left, f)\n",
    "                with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(Q_table_right, f)\n",
    "                with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "                    pickle.dump(episode_count, f)\n",
    "\n",
    "        # After taking an action, observe new state and reward\n",
    "        new_state_left = (discretize_grid(left_paddle_pos[1]), right_paddle_pos[1], discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "        new_state_right = (discretize_grid(left_paddle_pos[1]), right_paddle_pos[1], discretize_grid(ball_pos[0]), discretize_grid(ball_pos[1]), discretize_velocity(ball_velocity[0], ball_velocity[1]))\n",
    "\n",
    "        # Check if state has changed\n",
    "        if new_state_left != state_left or new_state_right != state_right or reward_applied_this_loop:\n",
    "\n",
    "            # Initialize Q-values-left for the new state if not already present\n",
    "            if new_state_left not in Q_table_left:\n",
    "                Q_table_left[new_state_left] = {action: 0 for action in action_space}\n",
    "\n",
    "            # Initialize Q-values-right for the new state if not already present\n",
    "            if new_state_right not in Q_table_right:\n",
    "                Q_table_right[new_state_right] = {action: 0 for action in action_space}\n",
    "\n",
    "            # Calculate the best next action for both agents\n",
    "            best_next_action_left = max(Q_table_left[new_state_left], key=Q_table_left[new_state_left].get)\n",
    "            best_next_action_right = max(Q_table_right[new_state_right], key=Q_table_right[new_state_right].get)\n",
    "\n",
    "            # Q-Learning update rule for both agents\n",
    "            Q_table_left[state_left][action_left] = (1 - alpha) * Q_table_left[state_left][action_left] + alpha * (reward_left + gamma * Q_table_left[new_state_left][best_next_action_left])\n",
    "            Q_table_right[state_right][action_right] = (1 - alpha) * Q_table_right[state_right][action_right] + alpha * (reward_right + gamma * Q_table_right[new_state_right][best_next_action_right])\n",
    "\n",
    "        # Update current state for next iteration\n",
    "        state_left = new_state_left\n",
    "        state_right = new_state_right\n",
    "\n",
    "        # Draw paddles, ball, and scores\n",
    "        window.fill((0, 0, 0))  # Clear screen\n",
    "        pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "        pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "        pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "        # Display scores\n",
    "        font = pygame.font.SysFont(None, 30)\n",
    "        score_display = font.render(f\"score: {left_score} - {right_score}\", True, (255, 255, 255))\n",
    "        window.blit(score_display, (width // 2 - 45, 10))\n",
    "\n",
    "        # Display episode count\n",
    "        font = pygame.font.SysFont(None, 30)\n",
    "        episode_display = font.render(f\"episodes played: {episode_count}\", True, (255, 255, 255))\n",
    "        window.blit(episode_display, (width // 2 - 100, 40))\n",
    "\n",
    "        # Display current epsilon\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #epsilon_display = font.render(f\"Epsilon: {epsilon:.4f}\", True, (255, 255, 255))\n",
    "        #window.blit(epsilon_display, (10, 70))\n",
    "\n",
    "        # Display current frame within game\n",
    "        #font = pygame.font.SysFont(None, 30)\n",
    "        #epsilon_display = font.render(f\"iterations_this_game: {iterations_this_game}\", True, (255, 255, 255))\n",
    "        #window.blit(epsilon_display, (10, 160))\n",
    "\n",
    "        pygame.display.update()\n",
    "\n",
    "        if episode_count > episodes_to_run and episodes_to_run > 0:\n",
    "            run = False\n",
    "            #@TODO one final pickle save?\n",
    "            \n",
    "    results = {\n",
    "        'config': {\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'epsilon': epsilon,\n",
    "            'epsilon_min': epsilon_min,\n",
    "            'epsilon_decay': epsilon_decay,\n",
    "            'GAME_BOARD_GRID_SIZE': GAME_BOARD_GRID_SIZE,\n",
    "            'reward_for_winning_episode': reward_for_winning_episode,\n",
    "            'reward_for_hitting_ball': reward_for_hitting_ball\n",
    "        },\n",
    "        'metrics': {\n",
    "            'KR_reward_events_left': KR_reward_events_left,\n",
    "            'KR_reward_events_right': KR_reward_events_right,\n",
    "            'KR_ball_hits_left': KR_ball_hits_left,\n",
    "            'KR_ball_hits_right': KR_ball_hits_right,\n",
    "            'KR_win_loss_ratio': KR_win_loss_ratio,\n",
    "            'KR_avg_episode_length': KR_avg_episode_length,\n",
    "            'KR_epsilon_values': KR_epsilon_values\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    " \n",
    "    \n",
    "#Constants\n",
    "DATA_FILE_PREFIX = 'v34-'\n",
    "DEBUG_OFF = 0\n",
    "DEBUG_INFO = 1\n",
    "DEBUG_DEBUG = 2\n",
    "DEBUG_LEVEL = DEBUG_OFF # Default debug level setting\n",
    "GAME_BOARD_GRID_SIZE = 50\n",
    "\n",
    "#Rewards\n",
    "reward_for_winning_episode = 1\n",
    "reward_for_hitting_ball = 1\n",
    "\n",
    "# Initialize epsilon for the epsilon-greedy policy\n",
    "epsilon = 1.0 #(orig 1.0)\n",
    "epsilon_min = 0.10 #(orig .01)\n",
    "epsilon_decay = 0.999995 #(orig .995)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "alpha = 0.15  # Learning rate (orig .1)\n",
    "gamma = 0.95  # Discount factor (orig .99)\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('AI Learns Pong')\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "#Let's Play some Pong!!!\n",
    "episodes_to_run = 0 #set to 0 for infinite runs\n",
    "results = play_de_game(DATA_FILE_PREFIX, episodes_to_run, alpha, gamma, epsilon, epsilon_min, epsilon_decay, GAME_BOARD_GRID_SIZE, reward_for_winning_episode, reward_for_hitting_ball)\n",
    "print('results: ', results)     \n",
    "    \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2739a9",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462cd2",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0693c8",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as ε-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2002e88",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e387d6",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05f84b",
   "metadata": {},
   "source": [
    "## Building intuition around training variables\n",
    "\n",
    "1. **Alpha (α) - Learning Rate**: \n",
    "    - **What it does**: Determines how much of the new Q-value estimate I adopt.\n",
    "    - **Intuition**: Think of it as a \"blending factor.\" If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. A value between 0 and 1 blends the old and new information.\n",
    "    - **Example**: If α is high (closer to 1), I will rapidly adapt to new strategies but may also forget useful past knowledge quickly.\n",
    "\n",
    "2. **Gamma (γ) - Discount Factor**: \n",
    "    - **What it does**: Influences how much future rewards contribute to the Q-value.\n",
    "    - **Intuition**: It's like a \"patience meter.\" A high γ makes me prioritize long-term reward over short-term reward.\n",
    "    - **Example**: If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "\n",
    "3. **Epsilon (ε) - Exploration Rate**: \n",
    "    - **What it does**: Controls the trade-off between exploration (trying new actions) and exploitation (sticking with known actions).\n",
    "    - **Intuition**: It's like the \"curiosity level.\" A high ε encourages me to try new things, while a low ε makes me stick to what I know.\n",
    "    - **Example**: If ε starts high and decays over time (ε-decay), I will initially explore a lot and gradually shift to exploiting my learned knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105212fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
