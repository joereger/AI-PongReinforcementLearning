{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03b6c56",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretty standard stuff here\n",
    "\n",
    "!mkdir PongReinforcementLearning\n",
    "!cd PongReinforcementLearning\n",
    "\n",
    "# Then, I set up a virtual environment (venv)\n",
    "python -m venv PongReinforcementLearningVENV\n",
    "!source PongReinforcementLearningVENV/bin/activate\n",
    "\n",
    "# Make the venv recognizable to Jupyter Notebooks.\n",
    "# This is the bridge that connects Jupyter to my isolated Python environment.\n",
    "%pip install ipyconfig\n",
    "python -m ipykernel install --user --name=PongReinforcementLearningVENV\n",
    "\n",
    "# Time to fire up Jupyter Notebook.\n",
    "# Make sure to select the new venv as the Python interpreter.\n",
    "jupyter notebook\n",
    "\n",
    "# Finally, installing some libs, i usually do these via the console but Jupyter's % operator usually works just fine\n",
    "%pip3 install pygame\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e54589",
   "metadata": {},
   "source": [
    "# See if I can run an external Pygame window from a Jupyter notebook on macosx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecbdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create external window\n",
    "win = pygame.display.set_mode((500, 500))\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    pygame.time.delay(100)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    # Game logic here (e.g., move a rectangle)\n",
    "    pygame.draw.rect(win, (255, 0, 0), (250, 250, 50, 50))\n",
    "    \n",
    "    pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae897a",
   "metadata": {},
   "source": [
    "**Well, it runs but shutdown isn't graceful.  The window pops up, draws a glorious red square.  But then simple window commands like \"close\" fail.  I had to Force Quit which then also brought the Jupyter notebook kernel to the ground.  This may wind up being a royal PITA but i'll give it a shot for now.  Worst case I'll switch to a simple python script run from the console.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09609",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8d21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np  \n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "#Helper function to load data from a pickle file\n",
    "def load_data_from_pickle_file(filename, default_value):\n",
    "    try: return pickle.load(open(filename, \"rb\")) if os.path.exists(filename) else default_value\n",
    "    except Exception as e: print(f\"Error loading {filename}: {e}\"); return default_value\n",
    "\n",
    "#Constants\n",
    "DATA_FILE_PREFIX = 'v11-'\n",
    "Q_TABLE_LEFT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_left.pkl'\n",
    "Q_TABLE_RIGHT_FILE = 'data/' + DATA_FILE_PREFIX + 'Q_table_right.pkl'\n",
    "EPISODE_COUNT_FILE = 'data/' + DATA_FILE_PREFIX + 'Episode_count.pkl'\n",
    "DEBUG_OFF = 0\n",
    "DEBUG_INFO = 1\n",
    "DEBUG_DEBUG = 2\n",
    "DEBUG_LEVEL = DEBUG_OFF # Default debug level setting\n",
    "\n",
    "# Initialize epsilon for the epsilon-greedy policy\n",
    "epsilon = 1.0 #(orig 1.0)\n",
    "epsilon_min = 0.05 #(orig .01)\n",
    "epsilon_decay = 0.999 #(orig .995)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "alpha = 0.5  # Learning rate (orig .1)\n",
    "gamma = 0.99589  # Discount factor (orig .995)\n",
    "\n",
    "#Rewards lookback period (for debugging, not training)\n",
    "reward_lookback_period = 100  # Number of episodes to average over\n",
    "recent_rewards_left = []\n",
    "recent_rewards_right = []\n",
    "\n",
    "# Initialize Q-tables\n",
    "Q_table_left = {}\n",
    "Q_table_right = {}\n",
    "\n",
    "#Q-table save frequency\n",
    "episode_count = 0  # Initialize episode count\n",
    "save_frequency = 100  # Save every 100 episodes\n",
    "\n",
    "#Load data from pickle\n",
    "Q_table_left = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "Q_table_right = load_data_from_pickle_file(Q_TABLE_RIGHT_FILE, {})\n",
    "episode_count = load_data_from_pickle_file(EPISODE_COUNT_FILE, 0)\n",
    "\n",
    "# Initialize scores\n",
    "left_score = 0\n",
    "right_score = 0\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1, 2]  # 0: Move Up, 1: Move Down, 2: Stay Still\n",
    "\n",
    "# Initialize reward\n",
    "reward = 0\n",
    "\n",
    "# Initialize iterations_this_game\n",
    "iterations_this_game = 0\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a window\n",
    "width, height = 800, 600  # Window dimensions\n",
    "window = pygame.display.set_mode((width, height))\n",
    "pygame.display.set_caption('Pong Game')\n",
    "\n",
    "# Initialize paddle and ball attributes\n",
    "paddle_width, paddle_height = 20, 100\n",
    "ball_radius = 15\n",
    "\n",
    "# Initial positions\n",
    "left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "ball_pos = [width // 2, height // 2]\n",
    "\n",
    "# Ball velocity\n",
    "ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "\n",
    "# Function to get random velocity\n",
    "def get_random_velocity():\n",
    "    angle_deg = random.uniform(30, 60)  # Random angle in degrees, between 30 and 60\n",
    "    angle_rad = math.radians(angle_deg)  # Convert angle to radians\n",
    "    velocity_magnitude = 4  # You can adjust this\n",
    "    velocity_x = velocity_magnitude * math.cos(angle_rad)\n",
    "    velocity_y = velocity_magnitude * math.sin(angle_rad)\n",
    "    # Randomly choose the direction (left/right and up/down)\n",
    "    velocity_x *= random.choice([-1, 1])\n",
    "    velocity_y *= random.choice([-1, 1])\n",
    "    # Return array representing random velocity in two dims\n",
    "    return [int(velocity_x), int(velocity_y)]\n",
    "\n",
    "# Main game loop\n",
    "run = True\n",
    "while run:\n",
    "    #pygame.time.delay(10)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            run = False\n",
    "            \n",
    "    #Track game loops in this episode/game and report to screen to get a sense of how many loops a game lasts\n",
    "    iterations_this_game += 1\n",
    "    \n",
    "    # Reset rewards to 0 at the beginning of each pass through the game loop\n",
    "    reward_left = 0\n",
    "    reward_right = 0\n",
    "            \n",
    "    # Create the state representation for both agents\n",
    "    state_left = (left_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "    state_right = (right_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "\n",
    "    # Initialize Q-values for the states if not already present\n",
    "    if state_left not in Q_table_left:\n",
    "        Q_table_left[state_left] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "    if state_right not in Q_table_right:\n",
    "        Q_table_right[state_right] = {action: np.random.uniform(-1, 1) for action in action_space}\n",
    "\n",
    "    # Choose an action for both agents using the epsilon-greedy policy\n",
    "    action_left = max(Q_table_left[state_left], key=Q_table_left[state_left].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "    action_right = max(Q_table_right[state_right], key=Q_table_right[state_right].get) if np.random.rand() >= epsilon else np.random.choice(action_space)\n",
    "   \n",
    "    # Manual human paddle movement with boundary checks\n",
    "    #keys = pygame.key.get_pressed()\n",
    "    #if keys[pygame.K_w] and left_paddle_pos[1] > 0:\n",
    "    #    left_paddle_pos[1] -= 5\n",
    "    #if keys[pygame.K_s] and left_paddle_pos[1] < height - paddle_height:\n",
    "    #    left_paddle_pos[1] += 5\n",
    "    #if keys[pygame.K_UP] and right_paddle_pos[1] > 0:\n",
    "    #    right_paddle_pos[1] -= 5\n",
    "    #if keys[pygame.K_DOWN] and right_paddle_pos[1] < height - paddle_height:\n",
    "    #    right_paddle_pos[1] += 5\n",
    "\n",
    "    #Left AI agent moves the paddle!!\n",
    "    if action_left == 0 and left_paddle_pos[1] > 0:  # Move Up\n",
    "        left_paddle_pos[1] -= 10\n",
    "    elif action_left == 1 and left_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "        left_paddle_pos[1] += 10\n",
    "    #elif action_left == 2: \n",
    "        # Stay Still, so no movement\n",
    "        \n",
    "    #Right AI agent moves the paddle!!\n",
    "    if action_right == 0 and right_paddle_pos[1] > 0:  # Move Up\n",
    "        right_paddle_pos[1] -= 10\n",
    "    elif action_right == 1 and right_paddle_pos[1] < height - paddle_height:  # Move Down\n",
    "        right_paddle_pos[1] += 10\n",
    "    #elif action_right == 2: \n",
    "        # Stay Still, so no movement\n",
    "\n",
    "    # Update ball position\n",
    "    ball_pos[0] += ball_velocity[0]\n",
    "    ball_pos[1] += ball_velocity[1]\n",
    "\n",
    "    # Collision detection with walls\n",
    "    if ball_pos[1] <= 0 or ball_pos[1] >= height:\n",
    "        ball_velocity[1] = -ball_velocity[1]\n",
    "\n",
    "    \n",
    "    # Collision detection with paddles\n",
    "    collision_offset = 5  # Define an offset to push the ball away from the paddle\n",
    "    if (left_paddle_pos[0] <= ball_pos[0] <= left_paddle_pos[0] + paddle_width and\n",
    "        left_paddle_pos[1] <= ball_pos[1] <= left_paddle_pos[1] + paddle_height):\n",
    "        ball_velocity[0] = -ball_velocity[0]\n",
    "        ball_pos[0] += collision_offset  # Push the ball away from the paddle\n",
    "        reward_left = 1  # Add reward for left agent\n",
    "    elif (right_paddle_pos[0] <= ball_pos[0] <= right_paddle_pos[0] + paddle_width and\n",
    "          right_paddle_pos[1] <= ball_pos[1] <= right_paddle_pos[1] + paddle_height):\n",
    "        ball_velocity[0] = -ball_velocity[0]\n",
    "        ball_pos[0] -= collision_offset  # Push the ball away from the paddle\n",
    "        reward_right = 1  # Add reward for right agent\n",
    "    \n",
    "    #Penalties for not exploring enough\n",
    "    #extreme_zones = [[0, height // 8], [7 * height // 8, height]]  # Define the extreme zones\n",
    "    #bonus = 0.1  # Define the bonus\n",
    "    #center_zone = [height // 4, 3 * height // 4]  # Define the center zone\n",
    "    #penalty = -0.1  # Define the penalty\n",
    "    # Apply penalty for center zone\n",
    "    #if center_zone[0] <= left_paddle_pos[1] <= center_zone[1]:\n",
    "    #    reward_left += penalty\n",
    "    #if center_zone[0] <= right_paddle_pos[1] <= center_zone[1]:\n",
    "    #    reward_right += penalty\n",
    "    # Apply bonus for extreme zones\n",
    "    #for zone in extreme_zones:\n",
    "    #    if zone[0] <= left_paddle_pos[1] <= zone[1]:\n",
    "    #        reward_left += bonus\n",
    "    #    if zone[0] <= right_paddle_pos[1] <= zone[1]:\n",
    "    #        reward_right += bonus\n",
    "        \n",
    "    # Ball reset, scoring, and immediate feedback game-over condition\n",
    "    if ball_pos[0] < 0:\n",
    "        # Reset paddle positions to the middle\n",
    "        left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "        right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "        #Reset the ball to the center in a random direction\n",
    "        ball_pos = [width // 2, height // 2]\n",
    "        #ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "        ball_velocity = get_random_velocity()\n",
    "        #Scoring\n",
    "        right_score += 1  # Right player scores\n",
    "        #Rewards\n",
    "        reward_left += -1  # Negative reward for the left agent\n",
    "        reward_right += 1  # Positive reward for the right agent\n",
    "        #Signal the end of an episode\n",
    "        episode_count += 1  # Increment episode count\n",
    "        iterations_this_game = 0\n",
    "        # Decay epsilon at the end of a game/episode\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        # Save the Q-tables every save_frequency episodes\n",
    "        if episode_count % save_frequency == 0:\n",
    "            with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "                pickle.dump(Q_table_left, f)\n",
    "            with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "                pickle.dump(Q_table_right, f)\n",
    "            with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "                pickle.dump(episode_count, f)\n",
    "    elif ball_pos[0] > width:\n",
    "        # Reset paddle positions to the middle\n",
    "        left_paddle_pos = [50, height // 2 - paddle_height // 2]\n",
    "        right_paddle_pos = [width - 50 - paddle_width, height // 2 - paddle_height // 2]\n",
    "        #Reset the ball to the center in a random direction\n",
    "        ball_pos = [width // 2, height // 2]\n",
    "        #ball_velocity = [random.choice([-4, 4]), random.choice([-4, 4])]\n",
    "        ball_velocity = get_random_velocity()\n",
    "        #Scoring\n",
    "        left_score += 1  # Left player scores\n",
    "        #Rewards\n",
    "        reward_left += 1  # Positive reward for the left agent\n",
    "        reward_right += -1  # Negative reward for the right agent\n",
    "        #Signal the end of an episode\n",
    "        episode_count += 1  # Increment episode count\n",
    "        iterations_this_game = 0\n",
    "        # Decay epsilon at the end of a game/episode\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        # Save the Q-tables every save_frequency episodes\n",
    "        if episode_count % save_frequency == 0:\n",
    "            with open(Q_TABLE_LEFT_FILE, \"wb\") as f:\n",
    "                pickle.dump(Q_table_left, f)\n",
    "            with open(Q_TABLE_RIGHT_FILE, \"wb\") as f:\n",
    "                pickle.dump(Q_table_right, f)\n",
    "            with open(EPISODE_COUNT_FILE, \"wb\") as f:\n",
    "                pickle.dump(episode_count, f)\n",
    "                \n",
    "    # After taking an action, observe new state and reward\n",
    "    new_state_left = (left_paddle_pos[1], right_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "    new_state_right = (left_paddle_pos[1], right_paddle_pos[1], ball_pos[0], ball_pos[1], ball_velocity[0], ball_velocity[1])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Debug: Print alpha, gamma, reward_left, and reward_right\n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): print(f\" \")\n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): print(f\"Debug: Alpha: {alpha}, Gamma: {gamma}, Reward Left: {reward_left}, Reward Right: {reward_right}\")\n",
    "\n",
    "    # Initialize Q-values-left for the new state if not already present\n",
    "    if new_state_left not in Q_table_left:\n",
    "        Q_table_left[new_state_left] = {action: 0 for action in action_space}\n",
    "        if (DEBUG_LEVEL>=DEBUG_DEBUG): print(\"Debug: Added new state to Q_table_left\")  # Debug\n",
    "\n",
    "    # Initialize Q-values-right for the new state if not already present\n",
    "    if new_state_right not in Q_table_right:\n",
    "        Q_table_right[new_state_right] = {action: 0 for action in action_space}\n",
    "        if (DEBUG_LEVEL>=DEBUG_DEBUG): print(\"Debug: Added new state to Q_table_right\")  # Debug\n",
    "\n",
    "    # Calculate the best next action for both agents\n",
    "    best_next_action_left = max(Q_table_left[new_state_left], key=Q_table_left[new_state_left].get)\n",
    "    best_next_action_right = max(Q_table_right[new_state_right], key=Q_table_right[new_state_right].get)\n",
    "\n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): print(f\"Debug: Best Next Action Left: {best_next_action_left}, Best Next Action Right: {best_next_action_right}\")\n",
    "\n",
    "    # Q-Learning update rule for both agents\n",
    "    Q_table_left[state_left][action_left] = (1 - alpha) * Q_table_left[state_left][action_left] + alpha * (reward_left + gamma * Q_table_left[new_state_left][best_next_action_left])\n",
    "    Q_table_right[state_right][action_right] = (1 - alpha) * Q_table_right[state_right][action_right] + alpha * (reward_right + gamma * Q_table_right[new_state_right][best_next_action_right])\n",
    "\n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): print(f\"Debug: Updated Q-Value Left: {Q_table_left[state_left][action_left]}, Updated Q-Value Right: {Q_table_right[state_right][action_right]}\")\n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): print(f\" \")\n",
    "    \n",
    "    # Update current state for next iteration\n",
    "    state_left = new_state_left\n",
    "    state_right = new_state_right\n",
    "        \n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): \n",
    "        # Append the reward of the current episode to the list\n",
    "        recent_rewards_left.append(reward_left)\n",
    "        recent_rewards_right.append(reward_right)\n",
    "        # Remove the oldest reward if the list grows too large\n",
    "        if len(recent_rewards_left) > reward_lookback_period:\n",
    "            del recent_rewards_left[0]\n",
    "        if len(recent_rewards_right) > reward_lookback_period:\n",
    "            del recent_rewards_right[0]\n",
    "        # Calculate the average reward\n",
    "        avg_reward_left = sum(recent_rewards_left) / len(recent_rewards_left)\n",
    "        avg_reward_right = sum(recent_rewards_right) / len(recent_rewards_right)\n",
    "\n",
    "    # Draw paddles, ball, and scores\n",
    "    window.fill((0, 0, 0))  # Clear screen\n",
    "    pygame.draw.rect(window, (255, 255, 255), left_paddle_pos + [paddle_width, paddle_height])\n",
    "    pygame.draw.rect(window, (255, 255, 255), right_paddle_pos + [paddle_width, paddle_height])\n",
    "    pygame.draw.circle(window, (255, 255, 255), ball_pos, ball_radius)\n",
    "\n",
    "    # Display scores\n",
    "    font = pygame.font.SysFont(None, 30)\n",
    "    score_display = font.render(f\"score: {left_score} - {right_score}\", True, (255, 255, 255))\n",
    "    window.blit(score_display, (width // 2 - 45, 10))\n",
    "    \n",
    "    # Display episode count\n",
    "    font = pygame.font.SysFont(None, 30)\n",
    "    episode_display = font.render(f\"iteration: {episode_count}\", True, (255, 255, 255))\n",
    "    window.blit(episode_display, (width // 2 - 50, 35))\n",
    "    \n",
    "    # Display current epsilon\n",
    "    #font = pygame.font.SysFont(None, 30)\n",
    "    #epsilon_display = font.render(f\"Epsilon: {epsilon:.4f}\", True, (255, 255, 255))\n",
    "    #window.blit(epsilon_display, (10, 70))\n",
    "\n",
    "    # Display average reward for left and right agents\n",
    "    #font = pygame.font.SysFont(None, 30)\n",
    "    #avg_reward_left_display = font.render(f\"Avg Reward Left: {avg_reward_left:.2f}\", True, (255, 255, 255))\n",
    "    #window.blit(avg_reward_left_display, (10, 100))\n",
    "    #avg_reward_right_display = font.render(f\"Avg Reward Right: {avg_reward_right:.2f}\", True, (255, 255, 255))\n",
    "    #window.blit(avg_reward_right_display, (10, 130))\n",
    "    \n",
    "    # Display current epsilon\n",
    "    #font = pygame.font.SysFont(None, 30)\n",
    "    #epsilon_display = font.render(f\"iterations_this_game: {iterations_this_game}\", True, (255, 255, 255))\n",
    "    #window.blit(epsilon_display, (10, 160))\n",
    "    \n",
    "    if (DEBUG_LEVEL>=DEBUG_DEBUG): \n",
    "        if Q_table_left[state_left][action_left] != 0:\n",
    "            print(f\"Episode: {episode_count}, Iteration: {iterations_this_game}\")\n",
    "            print(f\"Current State Left: {state_left}, Action Left: {action_left}, Q-Value: {Q_table_left[state_left][action_left]}\")\n",
    "            print(f\"New State Left: {new_state_left}, Best Next Action Left: {best_next_action_left}, Q-Value: {Q_table_left[new_state_left][best_next_action_left]}\")\n",
    "            print(f\"Reward Left: {reward_left}\")\n",
    "            print(f\"Current Epsilon: {epsilon}\")\n",
    "            #print(f\"Avg Reward Left: {avg_reward_left}\")\n",
    "            print(f\"----------\")\n",
    "            print(f\" \")\n",
    "\n",
    "\n",
    "    pygame.display.update()\n",
    "    \n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2739a9",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e462cd2",
   "metadata": {},
   "source": [
    "## Implementing Game Mechanics for Pong\n",
    "\n",
    "### 1. Initialize Pygame and Create Window\n",
    "- Initialized Pygame and created an 800x600 window for the game.\n",
    "\n",
    "### 2. Initialize Paddle and Ball Attributes\n",
    "- Defined the dimensions of the paddles and the ball. Initialized their starting positions.\n",
    "\n",
    "### 3. Paddle Movement\n",
    "- Implemented keyboard controls for moving the paddles up and down.\n",
    "\n",
    "### 4. Ball Movement and Collision Detection\n",
    "- Added logic for ball movement and collision detection with the walls and paddles.\n",
    "\n",
    "### 5. Ball Reset and Scoring\n",
    "- Implemented ball reset and scoring mechanics. The ball resets to the center after a point is scored.\n",
    "\n",
    "### 6. Paddle Boundaries\n",
    "- Added boundaries to prevent the paddles from moving out of the window.\n",
    "\n",
    "### 7. Game Over Conditions\n",
    "- Implemented immediate feedback game-over conditions. The game resets after each point, serving as an episode in RL terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0693c8",
   "metadata": {},
   "source": [
    "## Defining RL Elements for Pong\n",
    "\n",
    "### 1. State Representation\n",
    "- Decide how to represent the state of the game. Consider the trade-offs between granularity and computational complexity.\n",
    "\n",
    "### 2. Action Space\n",
    "- Define the set of actions I can take (e.g., move paddle up, move paddle down, stay still).\n",
    "\n",
    "### 3. Reward Structure\n",
    "- Design the rewards I receive for various outcomes (e.g., +1 for scoring, -1 for opponent scoring).\n",
    "\n",
    "### 4. Policy Initialization\n",
    "- Initialize my policy, which could be a Q-table, a neural network, or some other function mapping states to actions.\n",
    "\n",
    "### 5. Learning Algorithm\n",
    "- Choose and implement a learning algorithm (e.g., Q-learning, SARSA, Deep Q-Networks) to update my policy based on experiences.\n",
    "\n",
    "### 6. Exploration-Exploitation Strategy\n",
    "- Decide on a strategy for balancing exploration (trying new actions) and exploitation (sticking with known good actions), such as ε-greedy.\n",
    "\n",
    "### 7. Training Loop\n",
    "- Implement the training loop where I interact with the environment, update my policy, and optionally log metrics like average reward over time.\n",
    "\n",
    "### 8. Evaluation Metrics\n",
    "- Define metrics to evaluate my performance (e.g., average reward, win rate).\n",
    "\n",
    "### 9. Hyperparameter Tuning\n",
    "- Experiment with different learning rates, discount factors, and other hyperparameters to optimize performance.\n",
    "\n",
    "### 10. Testing and Validation\n",
    "- Test the trained agent to see how well it performs and validate that it is learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2002e88",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that aims to learn a policy, which tells an agent what action to take under what circumstances. It defines a function \\( Q(s, a) \\), representing the quality or the utility of taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. **Initialize Q-Table**: Create a table to store the Q-values for each state-action pair.\n",
    "2. **Policy**: Define how the agent chooses an action (e.g., \\(\\epsilon\\)-greedy).\n",
    "3. **Learning**: Update the Q-values using the Q-Learning update rule.\n",
    "4. **Training Loop**: Incorporate these elements into the game loop.\n",
    "\n",
    "The Q-table will be represented as a Python dictionary. The keys will be the states, and the values will be another dictionary mapping actions to Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e387d6",
   "metadata": {},
   "source": [
    "## max() reference\n",
    "\n",
    "| Iterable Type | What It Returns to `max()` | Example of Using `max()` |\n",
    "|---------------|----------------------------|--------------------------|\n",
    "| List          | Individual list elements   | `max([1, 2, 3])` returns `3` |\n",
    "| Tuple         | Individual tuple elements  | `max((1, 2, 3))` returns `3` |\n",
    "| String        | Individual characters     | `max(\"abc\")` returns `'c'` |\n",
    "| Set           | Individual set elements    | `max({1, 2, 3})` returns `3` |\n",
    "| Dictionary    | Dictionary keys           | `max({'a': 1, 'b': 2}, key=lambda k: k)` returns `'b'` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}.values())` returns `2` |\n",
    "|               |                            | `max({'a': 1, 'b': 2}, key=lambda k: {'a': 1, 'b': 2}[k])` returns `'b'` |\n",
    "| Numpy Array   | Individual array elements  | `import numpy as np; max(np.array([1, 2, 3]))` returns `3` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8624b7",
   "metadata": {},
   "source": [
    "## Building intuition around training variables\n",
    "\n",
    "1. **Alpha (α) - Learning Rate**: \n",
    "    - **What it does**: Determines how much of the new Q-value estimate I adopt.\n",
    "    - **Intuition**: Think of it as a \"blending factor.\" If α is 1, I consider only the most recent information. If α is 0, I learn nothing and stick to my prior knowledge. A value between 0 and 1 blends the old and new information.\n",
    "    - **Example**: If α is high (closer to 1), I will rapidly adapt to new strategies but may also forget useful past knowledge quickly.\n",
    "\n",
    "2. **Gamma (γ) - Discount Factor**: \n",
    "    - **What it does**: Influences how much future rewards contribute to the Q-value.\n",
    "    - **Intuition**: It's like a \"patience meter.\" A high γ makes me prioritize long-term reward over short-term reward.\n",
    "    - **Example**: If γ is close to 1, I will consider future rewards with greater weight, making me more strategic but potentially slower to train.\n",
    "\n",
    "3. **Epsilon (ε) - Exploration Rate**: \n",
    "    - **What it does**: Controls the trade-off between exploration (trying new actions) and exploitation (sticking with known actions).\n",
    "    - **Intuition**: It's like the \"curiosity level.\" A high ε encourages me to try new things, while a low ε makes me stick to what I know.\n",
    "    - **Example**: If ε starts high and decays over time (ε-decay), I will initially explore a lot and gradually shift to exploiting my learned knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95bb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PongReinforcementLearningVENV",
   "language": "python",
   "name": "pongreinforcementlearningvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
